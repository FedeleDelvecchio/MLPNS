{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lez15 15042021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTwfCRS/nazlsfPInF9WC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedeleDelvecchio/MLPNS/blob/main/Class/Lez15_15042021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOg7tbU5REtd"
      },
      "source": [
        "Graphviz package permette di vedere dangeoun graph (tipo albero) e altre cose.\n",
        "\n",
        "###**Il modello sklearn utilizza il gini index (non confondere con il gini coefficient), dove la probability del Gini index si interpreta come la stessa cosa della purezza, perchè ha comunque un'interpretazione probabilistica. Il Gini index è la variazione della purity** Il Gini index somma su tutte le classi, quindi mi consente di tenere conto di più di una classe.\n",
        "\n",
        "**Sample** è la dimensione del gruppo all'interno del nodo. Cioè posso richiedere che le le foglie dell'albero abbiamo una dimensione maggiore di 3 passeggeri (nell'esempio del titanic).\n",
        "\n",
        "https://github.com/fedhere/DSPS/blob/master/lab9/titanictree.ipynb vecchio nb per titanic (il pacchetto è cambiato).\n",
        "\n",
        "L'hyperparam depth controlla il numero di split che si possono fare: controllo il max numero di split in ogni branch. Si controlla prima di tutto la depth per controllare l'overfitting.\n",
        "\n",
        "Vedremo tree methods.\n",
        "\n",
        "**Overfitting**: può succedere che il modello sia overtrained, ovvero acquisisca conoscenza specifica mentre fa training, ma ciò che impara vale solo per il training set e il modello quindi non è generalizzabile. (L'abbiamo visto ieri).\n",
        "\n",
        "**La depth di un albero può servire per evitare questo overfitting.** \n",
        "\n",
        "##Controllare la degradazione dell'efficienza dal training al test set, e questo controllo si fa per ogni modello. \n",
        "\n",
        "Controllare la maximum depth per non ottenre delle foglie che contengono gruppi con una singole unità diciamo. \n",
        "\n",
        "Un altro modo più accurato sarebbe quello del tree pruning: costruire un albero molto complesso con una depth alta e poi torno indietro tagliando i rami inutili per prevenire l'overfitting. (Molto complicato da usare).\n",
        "\n",
        "Si può fare regression con tree model (e con tutti i CART). Tutto questo è dentro sklearn. Si può fare questo, ma è pericoloso fare regression con tree. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euD_p3QzYKl6"
      },
      "source": [
        "**Il problema con gli alberi è che non sono stabili: trees have a high variance of the results**. Dobbimo settare il random state all'interno della call di sklearn.tree. Questo equivale a settarlo prima della funzione ma chiamarlo nella sklearn è più elegante e sicuro dai bug che potrebbero esserci nel cambio di codice.\n",
        "\n",
        "La high variance deriva del fatto che non posso esaminare tutte le scelte possibili per ogni variabili, soprattutto per quelle continue (perchè ho inifinità di modi di dividere la variabile continue). Non si può riolvere il problema esplorando tutta la likelihood space per ottimizzare la posterior che sia l'ottimizzazione del problema. **Quindi si possono dettare dei random guess per inizializzare sclete che rendono il problema possiblei a livello computazionale.**\n",
        "\n",
        "Altro porblema quando faccio la scelta sul gini index, la faccio sul singolo nodo. Se scelgo un gini index non ottimale al primo nodo, al nodo successivo potrei avere un gini migliore: ottimizzazione locale.\n",
        "\n",
        "Questi 2 problemi rendono i single trees inutilizzabili.\n",
        "\n",
        "**Different trees lead to different results. Per questo motivo i single trees non sono utilizzabili.** \n",
        "##Per ovviare a questo problema si può far correre il codice molte volte ed esaminare i risultati scegliendo il risultato più comune.\n",
        "Questo tipo di metodo si chiama ENSAMBLE METHODS.\n",
        "\n",
        "I due ENSAMBLE METHODS che guardiamo oggi sono:\n",
        "\n",
        "for both method we have regressor and classifiers. \n",
        "\n",
        "- **RANDOM FORESTS** \n",
        "\n",
        "Tree runs in parallel. The number of tree is proportional to the number of variables and the number of observation. Each trees uses a random subset of observation or features, or both. Each trees is a small tree that uses fewer than the total number of features, fewer than the total number of observations. Then we end up eith a set of prediction statistically idependent (there is no covariance between the results).\n",
        "\n",
        "**The classifier is the majority vote, i.e. what was the class predicted mostly.** In the titanic example if I get 90 trees out of 100 giving the person to be in the survival class, I interpret it as the 90% for the person to survive. This in not all true because to have a probabilistic interpretation I will have to understand hat the posterior is based on the prior. The prior will be embedded on all of the coiche the tree does. So I don't have a robust probilistic interpretation of the result.\n",
        "\n",
        "The regression result is the average of all the result.\n",
        "\n",
        "**HYPERPARAMETER (for the random forest classifiers):** n_estimator: how many trees I will use in my forest. Easly we can choose an hundered of trees (only if we have an hundred of observation, otherwise we will end in an bias method).\n",
        "Bootstrap: wether or not I am gonna use a subsets of feature or observation at each tree or I am gonna use all feature in the observations. We will set always bootstrap=True.\n",
        "warm_start: allows to save partially trained model. It is used for hardly intense model. \n",
        "\n",
        "- **GRADIENT BOOSTED TREES**\n",
        "\n",
        "Trees runs in series and each tree uses (different) weights for the features, learnign the weights from the result of previous tree. And this weights are added to the next tree. So each tree learn what were the feature that were important to make the classification. Adding some randomness will converge the tree more rapidly. \n",
        "\n",
        "The last tree has the prediction, so unlike random forest, we don't to worry about the result.\n",
        "\n",
        "**HYPERPARAMETER (for gradient boosted regressor):** learning_rate (like in the gradient descent, to set how large is the step of the iteration during optimization), here it will mean how cloose I will inheriting the weights that I learned in the previous step. High learning rate will lead to a faster convergence, but it might lead to a local minimum. \n",
        "subsample: size of the variable that I can use at each step of the fraction of the sample to be used for fitting. Defalult values is 1, all of the data and feature at every tree, choosed with some stocasticity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7WxgoulU19D"
      },
      "source": [
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import os\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1Kfd83vU9GS"
      },
      "source": [
        "#serve per vedere un tree graph\n",
        "import graphviz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acvtkXpaPXLj"
      },
      "source": [
        "#ESERCIZIO \n",
        "\n",
        "https://github.com/fedhere/MLPNS2021/blob/main/CART/higgsbosonSearch.ipynb\n",
        "\n",
        "Dati presi da kaggle data challenge, sul bosone di Higgs. \n",
        "\n",
        "Use gradient boosted trees and random forest both classifier to predict the kind of paricle, and for regressor the mass of the particles.\n",
        "**Make a plot of the covariance to get a sense of the correlation between variable, altough in tree mehods we don't need to care about covariance.**\n",
        "\n",
        "In qesto ambito dell'higgs discovery, il gradient boosted trees si è dimostrato migliore delle neural netowrk ad LHC. Le NN hanno un'accurateza migliore, ma il gradient boosted trees consente di usare facilmente più tipi di variabili (numeriche e categoriche)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np97QoFMVYvz",
        "outputId": "cafacc9b-9738-4f39-c1fa-bffcc4ae032c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzZfkx_KqmNX",
        "outputId": "4f1783a2-0149-44a8-97bc-9bbe9768d16f"
      },
      "source": [
        "ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geBDNdj1aApd",
        "outputId": "20eae2cb-072a-4c2e-f3f5-1e65d4d9e15c"
      },
      "source": [
        "cd drive/MyDrive/MLPNS/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MLPNS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvGfnM24faU9",
        "outputId": "9ab7a55c-353e-48d2-d1e3-cf67075aea2f"
      },
      "source": [
        "ls kaggle.json"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj31OceFfea5"
      },
      "source": [
        "!chmod 600 kaggle.json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPR1yUFofgLS"
      },
      "source": [
        "envs = json.load(open(\"kaggle.json\", \"r\"))\n",
        "os.environ[\"KAGGLE_USERNAME\"] = envs['username']\n",
        "os.environ[\"KAGGLE_KEY\"] = envs['key']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNGxMu-LfwOn",
        "outputId": "6441e276-343f-4f41-e005-159d1961a9f9"
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                         title                                              size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-vaccine-myths                                 Reddit Vaccine Myths                              227KB  2021-05-03 13:54:12           4192  \n",
            "crowww/a-large-scale-fish-dataset                           A Large Scale Fish Dataset                          3GB  2021-04-28 17:03:01           2405  \n",
            "promptcloud/careerbuilder-job-listing-2020                  Careerbuilder Job Listing 2020                     42MB  2021-03-05 06:59:52            548  \n",
            "mathurinache/twitter-edge-nodes                             Twitter Edge Nodes                                342MB  2021-03-08 06:43:04            251  \n",
            "dhruvildave/wikibooks-dataset                               Wikibooks Dataset                                   1GB  2021-02-18 10:08:27           1484  \n",
            "imsparsh/musicnet-dataset                                   MusicNet Dataset                                   22GB  2021-02-18 14:12:19            848  \n",
            "alsgroup/end-als                                            End ALS Kaggle Challenge                           12GB  2021-04-08 12:16:37            572  \n",
            "nickuzmenkov/nih-chest-xrays-tfrecords                      NIH Chest X-rays TFRecords                         11GB  2021-03-09 04:49:23            387  \n",
            "simiotic/github-code-snippets                               GitHub Code Snippets                                7GB  2021-03-03 11:34:39             94  \n",
            "fatiimaezzahra/famous-iconic-women                          Famous Iconic Women                               838MB  2021-02-28 14:56:00            486  \n",
            "coloradokb/dandelionimages                                  DandelionImages                                     4GB  2021-02-19 20:03:47            217  \n",
            "mathurinache/the-lj-speech-dataset                          The LJ Speech Dataset                               3GB  2021-02-15 09:19:54            112  \n",
            "stuartjames/lights                                          LightS: Light Specularity Dataset                  18GB  2021-02-18 14:32:26             39  \n",
            "landrykezebou/lvzhdr-tone-mapping-benchmark-dataset-tmonet  LVZ-HDR Tone Mapping Benchmark Dataset (TMO-Net)   24GB  2021-03-01 05:03:40             57  \n",
            "imsparsh/accentdb-core-extended                             AccentDB - Core & Extended                          6GB  2021-02-17 14:22:54             49  \n",
            "nickuzmenkov/ranzcr-clip-kfold-tfrecords                    RANZCR CLiP KFold TFRecords                         2GB  2021-02-21 13:29:51             64  \n",
            "datasnaek/youtube-new                                       Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         138197  \n",
            "zynicide/wine-reviews                                       Wine Reviews                                       51MB  2017-11-27 17:08:04         135297  \n",
            "datasnaek/chess                                             Chess Game Dataset (Lichess)                        3MB  2017-09-04 03:09:09          17478  \n",
            "rtatman/188-million-us-wildfires                            1.88 Million US Wildfires                         168MB  2020-05-12 21:03:49          15323  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iwkJjggf6Mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670c309b-045b-4b60-d444-563561393194"
      },
      "source": [
        "!mkdir kaggleHiggs"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘kaggleHiggs’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x4wAdYyf9_x",
        "outputId": "921d6acf-3e3d-4731-e017-c52aa570b2fa"
      },
      "source": [
        "cd kaggleHiggs/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/MLPNS/kaggleHiggs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDt6VnBrmzc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faea8aab-5f63-4fb6-f1db-1c62587c98b8"
      },
      "source": [
        "ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HiggsBosonCompetition_AMSMetric_rev1.py  test.csv  training.csv\n",
            "random_submission.zip                    test.zip  training.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJoLa4zugHKt",
        "outputId": "eee7ef07-598e-4118-af3d-0b5cd71ec36a"
      },
      "source": [
        "!kaggle competitions download -c higgs-boson"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "test.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "HiggsBosonCompetition_AMSMetric_rev1.py: Skipping, found more recently modified local copy (use --force to force download)\n",
            "random_submission.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "training.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hv1nSOagUNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14b3fca-1970-44a8-e41c-1f1ef6bfb0ff"
      },
      "source": [
        "ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "HiggsBosonCompetition_AMSMetric_rev1.py  test.csv  training.csv\n",
            "random_submission.zip                    test.zip  training.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SD7DLTN3ml_",
        "outputId": "7a4d64d4-927b-4a19-8c21-0d2000a9669b"
      },
      "source": [
        "!unzip training.zip\n",
        "!unzip test.zip"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  training.zip\n",
            "replace training.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  test.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fT3uGFgiOw-"
      },
      "source": [
        "higgsdata = pd.read_csv(\"training.csv\")"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "4k_vOLPaj4KO",
        "outputId": "588789a4-f976-4cc1-9143-0822e83cb388"
      },
      "source": [
        "higgsdata"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EventId</th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100000</td>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.91</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100001</td>\n",
              "      <td>160.937</td>\n",
              "      <td>68.768</td>\n",
              "      <td>103.235</td>\n",
              "      <td>48.146</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.473</td>\n",
              "      <td>2.078</td>\n",
              "      <td>125.157</td>\n",
              "      <td>0.879</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>42.014</td>\n",
              "      <td>2.039</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>36.918</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.103</td>\n",
              "      <td>44.704</td>\n",
              "      <td>-1.916</td>\n",
              "      <td>164.546</td>\n",
              "      <td>1</td>\n",
              "      <td>46.226</td>\n",
              "      <td>0.725</td>\n",
              "      <td>1.158</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>46.226</td>\n",
              "      <td>2.233584</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100002</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>162.172</td>\n",
              "      <td>125.953</td>\n",
              "      <td>35.635</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.148</td>\n",
              "      <td>9.336</td>\n",
              "      <td>197.814</td>\n",
              "      <td>3.776</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>32.154</td>\n",
              "      <td>-0.705</td>\n",
              "      <td>-2.093</td>\n",
              "      <td>121.409</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.052</td>\n",
              "      <td>54.283</td>\n",
              "      <td>-2.186</td>\n",
              "      <td>260.414</td>\n",
              "      <td>1</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.053</td>\n",
              "      <td>-2.028</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.347389</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100003</td>\n",
              "      <td>143.905</td>\n",
              "      <td>81.417</td>\n",
              "      <td>80.943</td>\n",
              "      <td>0.414</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.310</td>\n",
              "      <td>0.414</td>\n",
              "      <td>75.968</td>\n",
              "      <td>2.354</td>\n",
              "      <td>-1.285</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>22.647</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.010</td>\n",
              "      <td>53.321</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>31.082</td>\n",
              "      <td>0.060</td>\n",
              "      <td>86.062</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>5.446378</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100004</td>\n",
              "      <td>175.864</td>\n",
              "      <td>16.915</td>\n",
              "      <td>134.805</td>\n",
              "      <td>16.405</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.891</td>\n",
              "      <td>16.405</td>\n",
              "      <td>57.983</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>28.209</td>\n",
              "      <td>-2.197</td>\n",
              "      <td>-2.231</td>\n",
              "      <td>29.774</td>\n",
              "      <td>0.798</td>\n",
              "      <td>1.569</td>\n",
              "      <td>2.723</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>53.131</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.245333</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249995</th>\n",
              "      <td>349995</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>71.989</td>\n",
              "      <td>36.548</td>\n",
              "      <td>5.042</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>1.392</td>\n",
              "      <td>5.042</td>\n",
              "      <td>55.892</td>\n",
              "      <td>1.258</td>\n",
              "      <td>-1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>24.754</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>-0.905</td>\n",
              "      <td>31.137</td>\n",
              "      <td>-0.950</td>\n",
              "      <td>0.380</td>\n",
              "      <td>46.520</td>\n",
              "      <td>2.859</td>\n",
              "      <td>144.665</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.505083</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249996</th>\n",
              "      <td>349996</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>58.179</td>\n",
              "      <td>68.083</td>\n",
              "      <td>22.439</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>2.585</td>\n",
              "      <td>22.439</td>\n",
              "      <td>50.618</td>\n",
              "      <td>1.162</td>\n",
              "      <td>-1.345</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>23.416</td>\n",
              "      <td>-1.609</td>\n",
              "      <td>2.776</td>\n",
              "      <td>27.202</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.042</td>\n",
              "      <td>46.737</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>80.408</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>2.497259</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249997</th>\n",
              "      <td>349997</td>\n",
              "      <td>105.457</td>\n",
              "      <td>60.526</td>\n",
              "      <td>75.839</td>\n",
              "      <td>39.757</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>2.390</td>\n",
              "      <td>22.183</td>\n",
              "      <td>120.462</td>\n",
              "      <td>1.202</td>\n",
              "      <td>0.529</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>35.636</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>-3.132</td>\n",
              "      <td>42.834</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.851</td>\n",
              "      <td>23.419</td>\n",
              "      <td>-2.890</td>\n",
              "      <td>198.907</td>\n",
              "      <td>1</td>\n",
              "      <td>41.992</td>\n",
              "      <td>1.800</td>\n",
              "      <td>-0.166</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>41.992</td>\n",
              "      <td>0.018636</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249998</th>\n",
              "      <td>349998</td>\n",
              "      <td>94.951</td>\n",
              "      <td>19.362</td>\n",
              "      <td>68.812</td>\n",
              "      <td>13.504</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>3.365</td>\n",
              "      <td>13.504</td>\n",
              "      <td>55.859</td>\n",
              "      <td>0.999</td>\n",
              "      <td>1.414</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>27.944</td>\n",
              "      <td>-2.211</td>\n",
              "      <td>2.792</td>\n",
              "      <td>27.915</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>12.150</td>\n",
              "      <td>0.811</td>\n",
              "      <td>112.718</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.681611</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249999</th>\n",
              "      <td>349999</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>72.756</td>\n",
              "      <td>70.831</td>\n",
              "      <td>7.479</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>2.025</td>\n",
              "      <td>7.479</td>\n",
              "      <td>83.240</td>\n",
              "      <td>0.936</td>\n",
              "      <td>-1.411</td>\n",
              "      <td>-999.0</td>\n",
              "      <td>43.003</td>\n",
              "      <td>1.685</td>\n",
              "      <td>2.653</td>\n",
              "      <td>40.236</td>\n",
              "      <td>1.490</td>\n",
              "      <td>0.637</td>\n",
              "      <td>40.729</td>\n",
              "      <td>-1.596</td>\n",
              "      <td>99.405</td>\n",
              "      <td>0</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>-999.00</td>\n",
              "      <td>-999.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.877474</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250000 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        EventId  DER_mass_MMC  ...    Weight  Label\n",
              "0        100000       138.470  ...  0.002653      s\n",
              "1        100001       160.937  ...  2.233584      b\n",
              "2        100002      -999.000  ...  2.347389      b\n",
              "3        100003       143.905  ...  5.446378      b\n",
              "4        100004       175.864  ...  6.245333      b\n",
              "...         ...           ...  ...       ...    ...\n",
              "249995   349995      -999.000  ...  4.505083      b\n",
              "249996   349996      -999.000  ...  2.497259      b\n",
              "249997   349997       105.457  ...  0.018636      s\n",
              "249998   349998        94.951  ...  1.681611      b\n",
              "249999   349999      -999.000  ...  1.877474      b\n",
              "\n",
              "[250000 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dZhCghJh3_Z"
      },
      "source": [
        "import sklearn\n",
        "import matplotlib as pl"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAfPapn7w-cd"
      },
      "source": [
        "Per vedere la **codipendenza** dei dati, possiamo usare:\n",
        "\n",
        "se i dati NON presentano covariance i plot sono tutti tondi ecc.. se c'è covariance ottengo delle righe e forme strane. Ci mette tanto tempo perchè ho un sacco di dati. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIB23Sbqw2-O"
      },
      "source": [
        "#pd.plotting.scatter_matrix(higgsdata)"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFDQPjU6xymR"
      },
      "source": [
        "Se facciamo un hist delle variabili mi accorgo che ci sono missing value codificati con i \"-999.00\" e questi devo rimuoverli altrimenti il modello li tratta come numeri. Vediamo quante ce ne sono per ognuna delle colonne e decido se togliere la colonna o togliere le osservaizoni che hanno alcune colonne -999.00.\n",
        "\n",
        "La cosa migliore da fare è sostiutire i -999.00 con un NaN in tutto il df e poi posso vedere quanti NaN ci sono e posso poi comodamenti rimuoverli con dropna. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "FbgdsPd4xvq9",
        "outputId": "6b628481-f9c6-4e01-8d3b-a771c6ed46ac"
      },
      "source": [
        "#ESEMPIO vedo quanti -999 ho per la colonna DER_deltaeta_jet_jet:\n",
        "pl.pyplot.hist(higgsdata.DER_deltaeta_jet_jet)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([177457.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,  72543.]),\n",
              " array([-999.    , -898.2497, -797.4994, -696.7491, -595.9988, -495.2485,\n",
              "        -394.4982, -293.7479, -192.9976,  -92.2473,    8.503 ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVpbg-OI1NFP"
      },
      "source": [
        "Rimpiazzo i -999.00 con dei NaN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XWjCjvh0mQn"
      },
      "source": [
        "higgsdata = higgsdata.replace(-999.00, np.nan)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qsmi2cT1Vrq",
        "outputId": "ea15cb74-b79f-4239-a2fc-ff78ac151b2b"
      },
      "source": [
        "higgsdata.isna().sum()"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EventId                             0\n",
              "DER_mass_MMC                    38114\n",
              "DER_mass_transverse_met_lep         0\n",
              "DER_mass_vis                        0\n",
              "DER_pt_h                            0\n",
              "DER_deltaeta_jet_jet           177457\n",
              "DER_mass_jet_jet               177457\n",
              "DER_prodeta_jet_jet            177457\n",
              "DER_deltar_tau_lep                  0\n",
              "DER_pt_tot                          0\n",
              "DER_sum_pt                          0\n",
              "DER_pt_ratio_lep_tau                0\n",
              "DER_met_phi_centrality              0\n",
              "DER_lep_eta_centrality         177457\n",
              "PRI_tau_pt                          0\n",
              "PRI_tau_eta                         0\n",
              "PRI_tau_phi                         0\n",
              "PRI_lep_pt                          0\n",
              "PRI_lep_eta                         0\n",
              "PRI_lep_phi                         0\n",
              "PRI_met                             0\n",
              "PRI_met_phi                         0\n",
              "PRI_met_sumet                       0\n",
              "PRI_jet_num                         0\n",
              "PRI_jet_leading_pt              99913\n",
              "PRI_jet_leading_eta             99913\n",
              "PRI_jet_leading_phi             99913\n",
              "PRI_jet_subleading_pt          177457\n",
              "PRI_jet_subleading_eta         177457\n",
              "PRI_jet_subleading_phi         177457\n",
              "PRI_jet_all_pt                      0\n",
              "Weight                              0\n",
              "Label                               0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL2ulqUu2EFT"
      },
      "source": [
        "###**Come detto avevamo un sacco di NaN, e decidiamo di toglierli**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "cs-ZH2Qq4HK6",
        "outputId": "67fcc7a4-e9fd-43cb-d761-1dce21264ba1"
      },
      "source": [
        "higgsdata"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EventId</th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100000</td>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.91</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.2</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.24</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100001</td>\n",
              "      <td>160.937</td>\n",
              "      <td>68.768</td>\n",
              "      <td>103.235</td>\n",
              "      <td>48.146</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.473</td>\n",
              "      <td>2.078</td>\n",
              "      <td>125.157</td>\n",
              "      <td>0.879</td>\n",
              "      <td>1.414</td>\n",
              "      <td>NaN</td>\n",
              "      <td>42.014</td>\n",
              "      <td>2.039</td>\n",
              "      <td>-3.011</td>\n",
              "      <td>36.918</td>\n",
              "      <td>0.501</td>\n",
              "      <td>0.103</td>\n",
              "      <td>44.704</td>\n",
              "      <td>-1.916</td>\n",
              "      <td>164.546</td>\n",
              "      <td>1</td>\n",
              "      <td>46.226</td>\n",
              "      <td>0.725</td>\n",
              "      <td>1.158</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>46.226</td>\n",
              "      <td>2.233584</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>162.172</td>\n",
              "      <td>125.953</td>\n",
              "      <td>35.635</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.148</td>\n",
              "      <td>9.336</td>\n",
              "      <td>197.814</td>\n",
              "      <td>3.776</td>\n",
              "      <td>1.414</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32.154</td>\n",
              "      <td>-0.705</td>\n",
              "      <td>-2.093</td>\n",
              "      <td>121.409</td>\n",
              "      <td>-0.953</td>\n",
              "      <td>1.052</td>\n",
              "      <td>54.283</td>\n",
              "      <td>-2.186</td>\n",
              "      <td>260.414</td>\n",
              "      <td>1</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.053</td>\n",
              "      <td>-2.028</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>44.251</td>\n",
              "      <td>2.347389</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>100003</td>\n",
              "      <td>143.905</td>\n",
              "      <td>81.417</td>\n",
              "      <td>80.943</td>\n",
              "      <td>0.414</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.310</td>\n",
              "      <td>0.414</td>\n",
              "      <td>75.968</td>\n",
              "      <td>2.354</td>\n",
              "      <td>-1.285</td>\n",
              "      <td>NaN</td>\n",
              "      <td>22.647</td>\n",
              "      <td>-1.655</td>\n",
              "      <td>0.010</td>\n",
              "      <td>53.321</td>\n",
              "      <td>-0.522</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>31.082</td>\n",
              "      <td>0.060</td>\n",
              "      <td>86.062</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>5.446378</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>100004</td>\n",
              "      <td>175.864</td>\n",
              "      <td>16.915</td>\n",
              "      <td>134.805</td>\n",
              "      <td>16.405</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.891</td>\n",
              "      <td>16.405</td>\n",
              "      <td>57.983</td>\n",
              "      <td>1.056</td>\n",
              "      <td>-1.385</td>\n",
              "      <td>NaN</td>\n",
              "      <td>28.209</td>\n",
              "      <td>-2.197</td>\n",
              "      <td>-2.231</td>\n",
              "      <td>29.774</td>\n",
              "      <td>0.798</td>\n",
              "      <td>1.569</td>\n",
              "      <td>2.723</td>\n",
              "      <td>-0.871</td>\n",
              "      <td>53.131</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000</td>\n",
              "      <td>6.245333</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249995</th>\n",
              "      <td>349995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>71.989</td>\n",
              "      <td>36.548</td>\n",
              "      <td>5.042</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.392</td>\n",
              "      <td>5.042</td>\n",
              "      <td>55.892</td>\n",
              "      <td>1.258</td>\n",
              "      <td>-1.414</td>\n",
              "      <td>NaN</td>\n",
              "      <td>24.754</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>-0.905</td>\n",
              "      <td>31.137</td>\n",
              "      <td>-0.950</td>\n",
              "      <td>0.380</td>\n",
              "      <td>46.520</td>\n",
              "      <td>2.859</td>\n",
              "      <td>144.665</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000</td>\n",
              "      <td>4.505083</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249996</th>\n",
              "      <td>349996</td>\n",
              "      <td>NaN</td>\n",
              "      <td>58.179</td>\n",
              "      <td>68.083</td>\n",
              "      <td>22.439</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.585</td>\n",
              "      <td>22.439</td>\n",
              "      <td>50.618</td>\n",
              "      <td>1.162</td>\n",
              "      <td>-1.345</td>\n",
              "      <td>NaN</td>\n",
              "      <td>23.416</td>\n",
              "      <td>-1.609</td>\n",
              "      <td>2.776</td>\n",
              "      <td>27.202</td>\n",
              "      <td>0.308</td>\n",
              "      <td>1.042</td>\n",
              "      <td>46.737</td>\n",
              "      <td>-0.867</td>\n",
              "      <td>80.408</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.000</td>\n",
              "      <td>2.497259</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249997</th>\n",
              "      <td>349997</td>\n",
              "      <td>105.457</td>\n",
              "      <td>60.526</td>\n",
              "      <td>75.839</td>\n",
              "      <td>39.757</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.390</td>\n",
              "      <td>22.183</td>\n",
              "      <td>120.462</td>\n",
              "      <td>1.202</td>\n",
              "      <td>0.529</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35.636</td>\n",
              "      <td>-0.266</td>\n",
              "      <td>-3.132</td>\n",
              "      <td>42.834</td>\n",
              "      <td>0.381</td>\n",
              "      <td>0.851</td>\n",
              "      <td>23.419</td>\n",
              "      <td>-2.890</td>\n",
              "      <td>198.907</td>\n",
              "      <td>1</td>\n",
              "      <td>41.992</td>\n",
              "      <td>1.800</td>\n",
              "      <td>-0.166</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>41.992</td>\n",
              "      <td>0.018636</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249998</th>\n",
              "      <td>349998</td>\n",
              "      <td>94.951</td>\n",
              "      <td>19.362</td>\n",
              "      <td>68.812</td>\n",
              "      <td>13.504</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.365</td>\n",
              "      <td>13.504</td>\n",
              "      <td>55.859</td>\n",
              "      <td>0.999</td>\n",
              "      <td>1.414</td>\n",
              "      <td>NaN</td>\n",
              "      <td>27.944</td>\n",
              "      <td>-2.211</td>\n",
              "      <td>2.792</td>\n",
              "      <td>27.915</td>\n",
              "      <td>-0.874</td>\n",
              "      <td>-0.296</td>\n",
              "      <td>12.150</td>\n",
              "      <td>0.811</td>\n",
              "      <td>112.718</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.681611</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249999</th>\n",
              "      <td>349999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>72.756</td>\n",
              "      <td>70.831</td>\n",
              "      <td>7.479</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.025</td>\n",
              "      <td>7.479</td>\n",
              "      <td>83.240</td>\n",
              "      <td>0.936</td>\n",
              "      <td>-1.411</td>\n",
              "      <td>NaN</td>\n",
              "      <td>43.003</td>\n",
              "      <td>1.685</td>\n",
              "      <td>2.653</td>\n",
              "      <td>40.236</td>\n",
              "      <td>1.490</td>\n",
              "      <td>0.637</td>\n",
              "      <td>40.729</td>\n",
              "      <td>-1.596</td>\n",
              "      <td>99.405</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.877474</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250000 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        EventId  DER_mass_MMC  ...    Weight  Label\n",
              "0        100000       138.470  ...  0.002653      s\n",
              "1        100001       160.937  ...  2.233584      b\n",
              "2        100002           NaN  ...  2.347389      b\n",
              "3        100003       143.905  ...  5.446378      b\n",
              "4        100004       175.864  ...  6.245333      b\n",
              "...         ...           ...  ...       ...    ...\n",
              "249995   349995           NaN  ...  4.505083      b\n",
              "249996   349996           NaN  ...  2.497259      b\n",
              "249997   349997       105.457  ...  0.018636      s\n",
              "249998   349998        94.951  ...  1.681611      b\n",
              "249999   349999           NaN  ...  1.877474      b\n",
              "\n",
              "[250000 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "H9idDE0Z2R_Y",
        "outputId": "a3a5d8ff-258b-4559-9938-35b76d8ee14a"
      },
      "source": [
        "higgsdata = higgsdata.dropna(axis=0, how='any')\n",
        "higgsdata"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EventId</th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100000</td>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.910</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.200</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.240</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>100005</td>\n",
              "      <td>89.744</td>\n",
              "      <td>13.550</td>\n",
              "      <td>59.149</td>\n",
              "      <td>116.344</td>\n",
              "      <td>2.636</td>\n",
              "      <td>284.584</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.362</td>\n",
              "      <td>61.619</td>\n",
              "      <td>278.876</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.975</td>\n",
              "      <td>53.651</td>\n",
              "      <td>0.371</td>\n",
              "      <td>1.329</td>\n",
              "      <td>31.565</td>\n",
              "      <td>-0.884</td>\n",
              "      <td>1.857</td>\n",
              "      <td>40.735</td>\n",
              "      <td>2.237</td>\n",
              "      <td>282.849</td>\n",
              "      <td>3</td>\n",
              "      <td>90.547</td>\n",
              "      <td>-2.412</td>\n",
              "      <td>-0.653</td>\n",
              "      <td>56.165</td>\n",
              "      <td>0.224</td>\n",
              "      <td>3.106</td>\n",
              "      <td>193.660</td>\n",
              "      <td>0.083414</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>100006</td>\n",
              "      <td>148.754</td>\n",
              "      <td>28.862</td>\n",
              "      <td>107.782</td>\n",
              "      <td>106.130</td>\n",
              "      <td>0.733</td>\n",
              "      <td>158.359</td>\n",
              "      <td>0.113</td>\n",
              "      <td>2.941</td>\n",
              "      <td>2.545</td>\n",
              "      <td>305.967</td>\n",
              "      <td>3.371</td>\n",
              "      <td>1.393</td>\n",
              "      <td>0.791</td>\n",
              "      <td>28.850</td>\n",
              "      <td>1.113</td>\n",
              "      <td>2.409</td>\n",
              "      <td>97.240</td>\n",
              "      <td>0.675</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>38.421</td>\n",
              "      <td>-1.443</td>\n",
              "      <td>294.074</td>\n",
              "      <td>2</td>\n",
              "      <td>123.010</td>\n",
              "      <td>0.864</td>\n",
              "      <td>1.450</td>\n",
              "      <td>56.867</td>\n",
              "      <td>0.131</td>\n",
              "      <td>-2.767</td>\n",
              "      <td>179.877</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>100011</td>\n",
              "      <td>114.744</td>\n",
              "      <td>10.286</td>\n",
              "      <td>75.712</td>\n",
              "      <td>30.816</td>\n",
              "      <td>2.563</td>\n",
              "      <td>252.599</td>\n",
              "      <td>-1.401</td>\n",
              "      <td>2.888</td>\n",
              "      <td>36.745</td>\n",
              "      <td>239.804</td>\n",
              "      <td>1.061</td>\n",
              "      <td>1.364</td>\n",
              "      <td>0.769</td>\n",
              "      <td>35.976</td>\n",
              "      <td>-0.669</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>38.188</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>2.502</td>\n",
              "      <td>22.385</td>\n",
              "      <td>2.148</td>\n",
              "      <td>290.547</td>\n",
              "      <td>3</td>\n",
              "      <td>76.773</td>\n",
              "      <td>-0.790</td>\n",
              "      <td>0.303</td>\n",
              "      <td>56.876</td>\n",
              "      <td>1.773</td>\n",
              "      <td>-2.079</td>\n",
              "      <td>165.640</td>\n",
              "      <td>0.307170</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>100023</td>\n",
              "      <td>141.481</td>\n",
              "      <td>0.736</td>\n",
              "      <td>111.581</td>\n",
              "      <td>174.075</td>\n",
              "      <td>1.955</td>\n",
              "      <td>364.344</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>1.335</td>\n",
              "      <td>6.663</td>\n",
              "      <td>440.859</td>\n",
              "      <td>0.652</td>\n",
              "      <td>1.042</td>\n",
              "      <td>0.207</td>\n",
              "      <td>98.565</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-1.506</td>\n",
              "      <td>64.285</td>\n",
              "      <td>1.405</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>17.960</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>454.785</td>\n",
              "      <td>2</td>\n",
              "      <td>195.533</td>\n",
              "      <td>1.156</td>\n",
              "      <td>1.416</td>\n",
              "      <td>82.477</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>-2.785</td>\n",
              "      <td>278.009</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249976</th>\n",
              "      <td>349976</td>\n",
              "      <td>137.371</td>\n",
              "      <td>4.640</td>\n",
              "      <td>92.603</td>\n",
              "      <td>107.121</td>\n",
              "      <td>3.189</td>\n",
              "      <td>322.430</td>\n",
              "      <td>-2.384</td>\n",
              "      <td>2.149</td>\n",
              "      <td>2.755</td>\n",
              "      <td>225.261</td>\n",
              "      <td>1.783</td>\n",
              "      <td>1.090</td>\n",
              "      <td>0.503</td>\n",
              "      <td>32.004</td>\n",
              "      <td>0.599</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>57.056</td>\n",
              "      <td>-0.924</td>\n",
              "      <td>1.503</td>\n",
              "      <td>42.299</td>\n",
              "      <td>1.408</td>\n",
              "      <td>228.186</td>\n",
              "      <td>2</td>\n",
              "      <td>85.132</td>\n",
              "      <td>1.991</td>\n",
              "      <td>-1.518</td>\n",
              "      <td>51.068</td>\n",
              "      <td>-1.197</td>\n",
              "      <td>-2.831</td>\n",
              "      <td>136.200</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249980</th>\n",
              "      <td>349980</td>\n",
              "      <td>119.934</td>\n",
              "      <td>20.078</td>\n",
              "      <td>88.751</td>\n",
              "      <td>35.227</td>\n",
              "      <td>0.660</td>\n",
              "      <td>111.491</td>\n",
              "      <td>1.836</td>\n",
              "      <td>2.800</td>\n",
              "      <td>18.532</td>\n",
              "      <td>189.198</td>\n",
              "      <td>1.951</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.000</td>\n",
              "      <td>25.844</td>\n",
              "      <td>1.159</td>\n",
              "      <td>0.823</td>\n",
              "      <td>50.416</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-1.558</td>\n",
              "      <td>2.113</td>\n",
              "      <td>1.116</td>\n",
              "      <td>237.326</td>\n",
              "      <td>2</td>\n",
              "      <td>69.219</td>\n",
              "      <td>-1.064</td>\n",
              "      <td>1.118</td>\n",
              "      <td>43.719</td>\n",
              "      <td>-1.725</td>\n",
              "      <td>-2.756</td>\n",
              "      <td>112.938</td>\n",
              "      <td>0.018636</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249985</th>\n",
              "      <td>349985</td>\n",
              "      <td>126.151</td>\n",
              "      <td>29.023</td>\n",
              "      <td>95.258</td>\n",
              "      <td>152.684</td>\n",
              "      <td>1.000</td>\n",
              "      <td>163.066</td>\n",
              "      <td>-0.240</td>\n",
              "      <td>1.504</td>\n",
              "      <td>24.642</td>\n",
              "      <td>327.502</td>\n",
              "      <td>2.163</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.027</td>\n",
              "      <td>42.495</td>\n",
              "      <td>1.980</td>\n",
              "      <td>2.239</td>\n",
              "      <td>91.908</td>\n",
              "      <td>0.851</td>\n",
              "      <td>-3.050</td>\n",
              "      <td>41.247</td>\n",
              "      <td>-2.575</td>\n",
              "      <td>402.114</td>\n",
              "      <td>2</td>\n",
              "      <td>158.904</td>\n",
              "      <td>0.401</td>\n",
              "      <td>0.034</td>\n",
              "      <td>34.196</td>\n",
              "      <td>-0.599</td>\n",
              "      <td>-2.525</td>\n",
              "      <td>193.099</td>\n",
              "      <td>0.018636</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249993</th>\n",
              "      <td>349993</td>\n",
              "      <td>130.075</td>\n",
              "      <td>3.918</td>\n",
              "      <td>66.781</td>\n",
              "      <td>77.369</td>\n",
              "      <td>0.936</td>\n",
              "      <td>322.296</td>\n",
              "      <td>-0.207</td>\n",
              "      <td>3.102</td>\n",
              "      <td>49.937</td>\n",
              "      <td>610.482</td>\n",
              "      <td>1.354</td>\n",
              "      <td>-0.634</td>\n",
              "      <td>0.000</td>\n",
              "      <td>27.364</td>\n",
              "      <td>2.403</td>\n",
              "      <td>1.348</td>\n",
              "      <td>37.052</td>\n",
              "      <td>1.775</td>\n",
              "      <td>-1.689</td>\n",
              "      <td>67.702</td>\n",
              "      <td>-1.768</td>\n",
              "      <td>694.010</td>\n",
              "      <td>3</td>\n",
              "      <td>155.864</td>\n",
              "      <td>-0.358</td>\n",
              "      <td>1.093</td>\n",
              "      <td>134.344</td>\n",
              "      <td>0.578</td>\n",
              "      <td>-2.215</td>\n",
              "      <td>546.066</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249994</th>\n",
              "      <td>349994</td>\n",
              "      <td>217.020</td>\n",
              "      <td>47.156</td>\n",
              "      <td>62.824</td>\n",
              "      <td>127.953</td>\n",
              "      <td>0.295</td>\n",
              "      <td>119.437</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>2.318</td>\n",
              "      <td>3.628</td>\n",
              "      <td>242.586</td>\n",
              "      <td>1.393</td>\n",
              "      <td>1.393</td>\n",
              "      <td>0.000</td>\n",
              "      <td>28.586</td>\n",
              "      <td>1.094</td>\n",
              "      <td>1.729</td>\n",
              "      <td>39.824</td>\n",
              "      <td>0.700</td>\n",
              "      <td>-0.555</td>\n",
              "      <td>97.737</td>\n",
              "      <td>0.220</td>\n",
              "      <td>271.082</td>\n",
              "      <td>2</td>\n",
              "      <td>141.752</td>\n",
              "      <td>0.237</td>\n",
              "      <td>3.126</td>\n",
              "      <td>32.423</td>\n",
              "      <td>-0.058</td>\n",
              "      <td>-1.137</td>\n",
              "      <td>174.176</td>\n",
              "      <td>0.064061</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68114 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        EventId  DER_mass_MMC  ...    Weight  Label\n",
              "0        100000       138.470  ...  0.002653      s\n",
              "5        100005        89.744  ...  0.083414      b\n",
              "6        100006       148.754  ...  0.002653      s\n",
              "11       100011       114.744  ...  0.307170      b\n",
              "23       100023       141.481  ...  0.001503      s\n",
              "...         ...           ...  ...       ...    ...\n",
              "249976   349976       137.371  ...  0.001503      s\n",
              "249980   349980       119.934  ...  0.018636      s\n",
              "249985   349985       126.151  ...  0.018636      s\n",
              "249993   349993       130.075  ...  0.001503      s\n",
              "249994   349994       217.020  ...  0.064061      b\n",
              "\n",
              "[68114 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K0LvL7mqTLQ",
        "outputId": "6da94bdf-e2ba-4485-f7ea-1c06f4f9dd08"
      },
      "source": [
        "higgsdata.isna().sum()"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EventId                        0\n",
              "DER_mass_MMC                   0\n",
              "DER_mass_transverse_met_lep    0\n",
              "DER_mass_vis                   0\n",
              "DER_pt_h                       0\n",
              "DER_deltaeta_jet_jet           0\n",
              "DER_mass_jet_jet               0\n",
              "DER_prodeta_jet_jet            0\n",
              "DER_deltar_tau_lep             0\n",
              "DER_pt_tot                     0\n",
              "DER_sum_pt                     0\n",
              "DER_pt_ratio_lep_tau           0\n",
              "DER_met_phi_centrality         0\n",
              "DER_lep_eta_centrality         0\n",
              "PRI_tau_pt                     0\n",
              "PRI_tau_eta                    0\n",
              "PRI_tau_phi                    0\n",
              "PRI_lep_pt                     0\n",
              "PRI_lep_eta                    0\n",
              "PRI_lep_phi                    0\n",
              "PRI_met                        0\n",
              "PRI_met_phi                    0\n",
              "PRI_met_sumet                  0\n",
              "PRI_jet_num                    0\n",
              "PRI_jet_leading_pt             0\n",
              "PRI_jet_leading_eta            0\n",
              "PRI_jet_leading_phi            0\n",
              "PRI_jet_subleading_pt          0\n",
              "PRI_jet_subleading_eta         0\n",
              "PRI_jet_subleading_phi         0\n",
              "PRI_jet_all_pt                 0\n",
              "Weight                         0\n",
              "Label                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC8Pulfq41XA"
      },
      "source": [
        "#si potevano togliere le colonne anche in maniera un po' più \"controllata\" in base a quanti NaN ci sono\n",
        "#remove=[]\n",
        "#for i in higgsdata.columns:\n",
        "# if higgsdata[i].isnull().sum()>40000\n",
        "#   remove=remove+[i]\n",
        "\n",
        "#higgsdata.drop(columns=remove, inplace=True)"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxVrD-etjOCL"
      },
      "source": [
        "Andrebbe rimosso anche l'EventID, perchè potrebbe modificare le scelte che faccio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "MK0gZDJo5ey8",
        "outputId": "2ce2ac33-5bac-4284-a3c2-570cdf3aa61b"
      },
      "source": [
        "higgsdata.drop(columns=['EventId'], inplace=True)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-186-5e1470676f47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhiggsdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'EventId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['EventId'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "8S7wYT1I50mi",
        "outputId": "6506ac5c-07e8-4652-adb1-3e0faa23617f"
      },
      "source": [
        "higgsdata.head()"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.910</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.200</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.240</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>89.744</td>\n",
              "      <td>13.550</td>\n",
              "      <td>59.149</td>\n",
              "      <td>116.344</td>\n",
              "      <td>2.636</td>\n",
              "      <td>284.584</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.362</td>\n",
              "      <td>61.619</td>\n",
              "      <td>278.876</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.975</td>\n",
              "      <td>53.651</td>\n",
              "      <td>0.371</td>\n",
              "      <td>1.329</td>\n",
              "      <td>31.565</td>\n",
              "      <td>-0.884</td>\n",
              "      <td>1.857</td>\n",
              "      <td>40.735</td>\n",
              "      <td>2.237</td>\n",
              "      <td>282.849</td>\n",
              "      <td>3</td>\n",
              "      <td>90.547</td>\n",
              "      <td>-2.412</td>\n",
              "      <td>-0.653</td>\n",
              "      <td>56.165</td>\n",
              "      <td>0.224</td>\n",
              "      <td>3.106</td>\n",
              "      <td>193.660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>148.754</td>\n",
              "      <td>28.862</td>\n",
              "      <td>107.782</td>\n",
              "      <td>106.130</td>\n",
              "      <td>0.733</td>\n",
              "      <td>158.359</td>\n",
              "      <td>0.113</td>\n",
              "      <td>2.941</td>\n",
              "      <td>2.545</td>\n",
              "      <td>305.967</td>\n",
              "      <td>3.371</td>\n",
              "      <td>1.393</td>\n",
              "      <td>0.791</td>\n",
              "      <td>28.850</td>\n",
              "      <td>1.113</td>\n",
              "      <td>2.409</td>\n",
              "      <td>97.240</td>\n",
              "      <td>0.675</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>38.421</td>\n",
              "      <td>-1.443</td>\n",
              "      <td>294.074</td>\n",
              "      <td>2</td>\n",
              "      <td>123.010</td>\n",
              "      <td>0.864</td>\n",
              "      <td>1.450</td>\n",
              "      <td>56.867</td>\n",
              "      <td>0.131</td>\n",
              "      <td>-2.767</td>\n",
              "      <td>179.877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>114.744</td>\n",
              "      <td>10.286</td>\n",
              "      <td>75.712</td>\n",
              "      <td>30.816</td>\n",
              "      <td>2.563</td>\n",
              "      <td>252.599</td>\n",
              "      <td>-1.401</td>\n",
              "      <td>2.888</td>\n",
              "      <td>36.745</td>\n",
              "      <td>239.804</td>\n",
              "      <td>1.061</td>\n",
              "      <td>1.364</td>\n",
              "      <td>0.769</td>\n",
              "      <td>35.976</td>\n",
              "      <td>-0.669</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>38.188</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>2.502</td>\n",
              "      <td>22.385</td>\n",
              "      <td>2.148</td>\n",
              "      <td>290.547</td>\n",
              "      <td>3</td>\n",
              "      <td>76.773</td>\n",
              "      <td>-0.790</td>\n",
              "      <td>0.303</td>\n",
              "      <td>56.876</td>\n",
              "      <td>1.773</td>\n",
              "      <td>-2.079</td>\n",
              "      <td>165.640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>141.481</td>\n",
              "      <td>0.736</td>\n",
              "      <td>111.581</td>\n",
              "      <td>174.075</td>\n",
              "      <td>1.955</td>\n",
              "      <td>364.344</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>1.335</td>\n",
              "      <td>6.663</td>\n",
              "      <td>440.859</td>\n",
              "      <td>0.652</td>\n",
              "      <td>1.042</td>\n",
              "      <td>0.207</td>\n",
              "      <td>98.565</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-1.506</td>\n",
              "      <td>64.285</td>\n",
              "      <td>1.405</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>17.960</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>454.785</td>\n",
              "      <td>2</td>\n",
              "      <td>195.533</td>\n",
              "      <td>1.156</td>\n",
              "      <td>1.416</td>\n",
              "      <td>82.477</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>-2.785</td>\n",
              "      <td>278.009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    DER_mass_MMC  ...  PRI_jet_all_pt\n",
              "0        138.470  ...         113.497\n",
              "5         89.744  ...         193.660\n",
              "6        148.754  ...         179.877\n",
              "11       114.744  ...         165.640\n",
              "23       141.481  ...         278.009\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roe4AvsVge6K"
      },
      "source": [
        "Adesso vorrei rimuovere la colonna label e weight dal dataframe, ma prima me le salvo in due vettori. This will be my target variables for, respectively, classification and regression.\n",
        "\n",
        "Per evitare che le persone possano imbrogliare, o creare modello specifici per questo set di dati, i dati kaggle sono divisi in train e test set, in cui i dati del test non hanno le label in questo caso. Siccome il test set è blinded, non posso sapere se il mio modello funziona bene o no. Quindi devo prendere il training set e dividerlo nel mio train e test set e provare il mio modello sui set che ho creato io, e poi applicare il modello al test set di kaggle. Ecco perchè utilizzamo solo il training set per il momento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRhR6S3bgDFa",
        "outputId": "bfbfe2aa-346f-4219-8f71-45d672e68141"
      },
      "source": [
        "label = np.array(higgsdata.Label)\n",
        "label"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['s', 'b', 's', ..., 's', 's', 'b'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO-aLfiVgU2f",
        "outputId": "0f8d8240-e1d2-4fc1-8b30-66dd5ecebc6c"
      },
      "source": [
        "weight = np.array(higgsdata.Weight)\n",
        "weight"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00265331, 0.08341403, 0.00265331, ..., 0.01863612, 0.0015027 ,\n",
              "       0.06406078])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jS28g2UofthD"
      },
      "source": [
        "higgsdata = higgsdata.drop(labels=['Label'], axis=1)\n",
        "higgsdata = higgsdata.drop(labels=['Weight'], axis=1)"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "1KkiD8i3klag",
        "outputId": "b33705cc-1cff-43ea-a035-66300deced66"
      },
      "source": [
        "higgsdata.head()"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.910</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.200</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.240</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>89.744</td>\n",
              "      <td>13.550</td>\n",
              "      <td>59.149</td>\n",
              "      <td>116.344</td>\n",
              "      <td>2.636</td>\n",
              "      <td>284.584</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.362</td>\n",
              "      <td>61.619</td>\n",
              "      <td>278.876</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.975</td>\n",
              "      <td>53.651</td>\n",
              "      <td>0.371</td>\n",
              "      <td>1.329</td>\n",
              "      <td>31.565</td>\n",
              "      <td>-0.884</td>\n",
              "      <td>1.857</td>\n",
              "      <td>40.735</td>\n",
              "      <td>2.237</td>\n",
              "      <td>282.849</td>\n",
              "      <td>3</td>\n",
              "      <td>90.547</td>\n",
              "      <td>-2.412</td>\n",
              "      <td>-0.653</td>\n",
              "      <td>56.165</td>\n",
              "      <td>0.224</td>\n",
              "      <td>3.106</td>\n",
              "      <td>193.660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>148.754</td>\n",
              "      <td>28.862</td>\n",
              "      <td>107.782</td>\n",
              "      <td>106.130</td>\n",
              "      <td>0.733</td>\n",
              "      <td>158.359</td>\n",
              "      <td>0.113</td>\n",
              "      <td>2.941</td>\n",
              "      <td>2.545</td>\n",
              "      <td>305.967</td>\n",
              "      <td>3.371</td>\n",
              "      <td>1.393</td>\n",
              "      <td>0.791</td>\n",
              "      <td>28.850</td>\n",
              "      <td>1.113</td>\n",
              "      <td>2.409</td>\n",
              "      <td>97.240</td>\n",
              "      <td>0.675</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>38.421</td>\n",
              "      <td>-1.443</td>\n",
              "      <td>294.074</td>\n",
              "      <td>2</td>\n",
              "      <td>123.010</td>\n",
              "      <td>0.864</td>\n",
              "      <td>1.450</td>\n",
              "      <td>56.867</td>\n",
              "      <td>0.131</td>\n",
              "      <td>-2.767</td>\n",
              "      <td>179.877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>114.744</td>\n",
              "      <td>10.286</td>\n",
              "      <td>75.712</td>\n",
              "      <td>30.816</td>\n",
              "      <td>2.563</td>\n",
              "      <td>252.599</td>\n",
              "      <td>-1.401</td>\n",
              "      <td>2.888</td>\n",
              "      <td>36.745</td>\n",
              "      <td>239.804</td>\n",
              "      <td>1.061</td>\n",
              "      <td>1.364</td>\n",
              "      <td>0.769</td>\n",
              "      <td>35.976</td>\n",
              "      <td>-0.669</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>38.188</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>2.502</td>\n",
              "      <td>22.385</td>\n",
              "      <td>2.148</td>\n",
              "      <td>290.547</td>\n",
              "      <td>3</td>\n",
              "      <td>76.773</td>\n",
              "      <td>-0.790</td>\n",
              "      <td>0.303</td>\n",
              "      <td>56.876</td>\n",
              "      <td>1.773</td>\n",
              "      <td>-2.079</td>\n",
              "      <td>165.640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>141.481</td>\n",
              "      <td>0.736</td>\n",
              "      <td>111.581</td>\n",
              "      <td>174.075</td>\n",
              "      <td>1.955</td>\n",
              "      <td>364.344</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>1.335</td>\n",
              "      <td>6.663</td>\n",
              "      <td>440.859</td>\n",
              "      <td>0.652</td>\n",
              "      <td>1.042</td>\n",
              "      <td>0.207</td>\n",
              "      <td>98.565</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-1.506</td>\n",
              "      <td>64.285</td>\n",
              "      <td>1.405</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>17.960</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>454.785</td>\n",
              "      <td>2</td>\n",
              "      <td>195.533</td>\n",
              "      <td>1.156</td>\n",
              "      <td>1.416</td>\n",
              "      <td>82.477</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>-2.785</td>\n",
              "      <td>278.009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    DER_mass_MMC  ...  PRI_jet_all_pt\n",
              "0        138.470  ...         113.497\n",
              "5         89.744  ...         193.660\n",
              "6        148.754  ...         179.877\n",
              "11       114.744  ...         165.640\n",
              "23       141.481  ...         278.009\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nackp_ulC4kq"
      },
      "source": [
        "#**OGNI VOLTA CHE TOLGO DELLE OSSERVAZIONI, CALCOLO QUANTA PERCENTUALE DI OSSERVAIZONI HO ELIMINIATO DAL MIO DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgwaxbq5stLQ"
      },
      "source": [
        "higgsdata.Label.unique ci permette di sapere quali sono gli unique value in this categorical, ovvero ritorna gli elementi che compaiono almeno una volta nella colonna, ad esempio. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG9J0M6khyWL"
      },
      "source": [
        "#Adesso, prima di creare il train_test_split dobbiamo ricordarci di standardizzare i dati con sklearn.preprocessing.\n",
        "from sklearn import preprocessing\n",
        "\n",
        "higgsdata_scaled = pd.DataFrame(sklearn.preprocessing.scale(higgsdata[higgsdata.columns],\n",
        "                                                            axis=0), columns = higgsdata.columns)\n"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDLY-92eqrAj"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Per splittare i dati va anche bene 75% e 25% se sono molto grandi. \n",
        "\n",
        "X_traindata, X_testdata, y_trainlabel, y_testlabel = train_test_split(higgsdata_scaled[higgsdata_scaled.columns], \n",
        "                    label, train_size=0.75, test_size=0.25, random_state=1) "
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "PkcWiiEk74rF",
        "outputId": "4d14ad53-c445-4374-9631-9437caaa526f"
      },
      "source": [
        "higgsdata_scaled.head()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.257671</td>\n",
              "      <td>0.532835</td>\n",
              "      <td>0.474611</td>\n",
              "      <td>-1.093454</td>\n",
              "      <td>-0.870648</td>\n",
              "      <td>-0.632428</td>\n",
              "      <td>0.987313</td>\n",
              "      <td>1.378045</td>\n",
              "      <td>0.515495</td>\n",
              "      <td>-0.643135</td>\n",
              "      <td>0.092057</td>\n",
              "      <td>0.820325</td>\n",
              "      <td>-0.662199</td>\n",
              "      <td>-0.440631</td>\n",
              "      <td>0.865134</td>\n",
              "      <td>0.208926</td>\n",
              "      <td>-0.028037</td>\n",
              "      <td>1.901426</td>\n",
              "      <td>-1.352776</td>\n",
              "      <td>-0.893300</td>\n",
              "      <td>-0.156564</td>\n",
              "      <td>-0.542982</td>\n",
              "      <td>-0.660444</td>\n",
              "      <td>-0.578666</td>\n",
              "      <td>1.234399</td>\n",
              "      <td>0.251795</td>\n",
              "      <td>-0.367844</td>\n",
              "      <td>0.612965</td>\n",
              "      <td>-1.359329</td>\n",
              "      <td>-0.645152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.544388</td>\n",
              "      <td>-0.706350</td>\n",
              "      <td>-0.475044</td>\n",
              "      <td>0.064383</td>\n",
              "      <td>0.115251</td>\n",
              "      <td>-0.233525</td>\n",
              "      <td>0.094923</td>\n",
              "      <td>-0.819132</td>\n",
              "      <td>1.212110</td>\n",
              "      <td>-0.020337</td>\n",
              "      <td>-0.813244</td>\n",
              "      <td>-0.171167</td>\n",
              "      <td>1.282989</td>\n",
              "      <td>0.267314</td>\n",
              "      <td>0.317413</td>\n",
              "      <td>0.730285</td>\n",
              "      <td>-0.724632</td>\n",
              "      <td>-0.735232</td>\n",
              "      <td>0.999809</td>\n",
              "      <td>-0.333197</td>\n",
              "      <td>1.229416</td>\n",
              "      <td>-0.370559</td>\n",
              "      <td>1.514133</td>\n",
              "      <td>-0.239608</td>\n",
              "      <td>-1.377788</td>\n",
              "      <td>-0.354529</td>\n",
              "      <td>-0.054320</td>\n",
              "      <td>0.115912</td>\n",
              "      <td>1.711823</td>\n",
              "      <td>0.094961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.426952</td>\n",
              "      <td>-0.208400</td>\n",
              "      <td>0.719034</td>\n",
              "      <td>-0.069451</td>\n",
              "      <td>-0.971751</td>\n",
              "      <td>-0.548472</td>\n",
              "      <td>0.276686</td>\n",
              "      <td>1.219260</td>\n",
              "      <td>-0.877769</td>\n",
              "      <td>0.187665</td>\n",
              "      <td>1.721416</td>\n",
              "      <td>0.817081</td>\n",
              "      <td>0.821164</td>\n",
              "      <td>-0.568252</td>\n",
              "      <td>0.946529</td>\n",
              "      <td>1.324237</td>\n",
              "      <td>1.555856</td>\n",
              "      <td>0.566811</td>\n",
              "      <td>-0.555177</td>\n",
              "      <td>-0.387401</td>\n",
              "      <td>-0.799385</td>\n",
              "      <td>-0.290304</td>\n",
              "      <td>-0.660444</td>\n",
              "      <td>0.236631</td>\n",
              "      <td>0.498040</td>\n",
              "      <td>0.807823</td>\n",
              "      <td>-0.032535</td>\n",
              "      <td>0.070414</td>\n",
              "      <td>-1.520013</td>\n",
              "      <td>-0.032292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.132873</td>\n",
              "      <td>-0.812497</td>\n",
              "      <td>-0.068375</td>\n",
              "      <td>-1.056294</td>\n",
              "      <td>0.073553</td>\n",
              "      <td>-0.313332</td>\n",
              "      <td>-0.144736</td>\n",
              "      <td>1.150840</td>\n",
              "      <td>0.332135</td>\n",
              "      <td>-0.320326</td>\n",
              "      <td>-0.382452</td>\n",
              "      <td>0.785725</td>\n",
              "      <td>0.765945</td>\n",
              "      <td>-0.328171</td>\n",
              "      <td>-0.564367</td>\n",
              "      <td>-0.188692</td>\n",
              "      <td>-0.494656</td>\n",
              "      <td>-0.134738</td>\n",
              "      <td>1.355093</td>\n",
              "      <td>-0.763036</td>\n",
              "      <td>1.180350</td>\n",
              "      <td>-0.315521</td>\n",
              "      <td>1.514133</td>\n",
              "      <td>-0.441676</td>\n",
              "      <td>-0.449036</td>\n",
              "      <td>0.173863</td>\n",
              "      <td>-0.032256</td>\n",
              "      <td>0.873721</td>\n",
              "      <td>-1.141415</td>\n",
              "      <td>-0.163737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.307234</td>\n",
              "      <td>-1.123065</td>\n",
              "      <td>0.812311</td>\n",
              "      <td>0.820835</td>\n",
              "      <td>-0.273740</td>\n",
              "      <td>-0.034514</td>\n",
              "      <td>-0.011685</td>\n",
              "      <td>-0.853987</td>\n",
              "      <td>-0.732085</td>\n",
              "      <td>1.223348</td>\n",
              "      <td>-0.754955</td>\n",
              "      <td>0.437568</td>\n",
              "      <td>-0.644630</td>\n",
              "      <td>1.780505</td>\n",
              "      <td>0.163949</td>\n",
              "      <td>-0.828841</td>\n",
              "      <td>0.411532</td>\n",
              "      <td>1.176491</td>\n",
              "      <td>-0.547466</td>\n",
              "      <td>-0.866690</td>\n",
              "      <td>-0.540272</td>\n",
              "      <td>0.858731</td>\n",
              "      <td>-0.660444</td>\n",
              "      <td>1.300559</td>\n",
              "      <td>0.665238</td>\n",
              "      <td>0.789031</td>\n",
              "      <td>0.762212</td>\n",
              "      <td>-0.384076</td>\n",
              "      <td>-1.529918</td>\n",
              "      <td>0.873721</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   DER_mass_MMC  ...  PRI_jet_all_pt\n",
              "0      0.257671  ...       -0.645152\n",
              "1     -0.544388  ...        0.094961\n",
              "2      0.426952  ...       -0.032292\n",
              "3     -0.132873  ...       -0.163737\n",
              "4      0.307234  ...        0.873721\n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCKzNauqkLWo",
        "outputId": "10203435-a3cc-4612-9adf-a59ea07672c2"
      },
      "source": [
        "X_traindata.shape"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51085, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa5AbLrVkRz2",
        "outputId": "f3e35536-4b5d-478b-eed2-287009e77121"
      },
      "source": [
        "X_testdata.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17029, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX8OrQijkT9F",
        "outputId": "5a2d125a-ff4f-411b-c079-4cf2f65f3c41"
      },
      "source": [
        "y_trainlabel.shape"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(51085,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOhlXmy6kXN8",
        "outputId": "b7f85bc0-0015-4711-de43-8c18a982c3fc"
      },
      "source": [
        "y_testlabel.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17029,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaQpxEwXmBSt"
      },
      "source": [
        "Adesso costruiamo i modelli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkPnOZvLils-",
        "outputId": "35b02d83-aac3-47ba-bf13-c1909731d20f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Look at parameters used by our current forest\n",
        "rf = RandomForestClassifier(random_state = 1)\n",
        "print('Parameters currently in use:\\n')\n",
        "print(rf.get_params())"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 1, 'verbose': 0, 'warm_start': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIM9_T8Zy57H"
      },
      "source": [
        "Facciamo RandomForestClassifier e GradientBoostingClassifier, e giochiamo un po' con gli hyperparameters. Iniziamo usando prima il modello random forest: \n",
        "\n",
        "- **Per prima cosa usiamo RandomForestClassifier per la classificazione della target variable 'label' per la classificazione s o b.**\n",
        "- Dopo aver fatto il modello, fittiamo il modello ai dati e controlliamo il risultato del modello usando la proprietà dell'oggetto random forest che è score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff6VIAzXmWaq"
      },
      "source": [
        "**Quindi costruiamo il gradient boosting model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1cyrIXzksLw"
      },
      "source": [
        "gbt = GradientBoostingClassifier(random_state=12)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNhUMhj6lCZX",
        "outputId": "1b5f6674-e027-441d-8d9f-489ebc625269"
      },
      "source": [
        "gbt.fit(X_traindata, y_trainlabel)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=12, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRwGU8dtlC9Q",
        "outputId": "61a63416-4670-4595-c93f-59d0b0e4a90b"
      },
      "source": [
        "gbt.score(X_traindata, y_trainlabel), gbt.score(X_testdata, y_testlabel)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8469217970049917, 0.8412707733865759)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvD7IpjV4SD1"
      },
      "source": [
        "###**SI POTREBBE ANCHE USARE kNN TO INFILL THE MISSING VALUE, ANZICHÈ RIMUOVERE I NaN**\n",
        "\n",
        "Quindi, giusto per provare, riprendo i dati dall'inizo. Ci metto i NaN e anzichè rimuoverli uso un kNN imputer per rimepire gli spazi NaN. **Prima di usare kNN, devo normalizzare i dati!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OAMczDrs9px"
      },
      "source": [
        "higgsdata2 = pd.read_csv(\"training.csv\")"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnzAR-4Us9pz"
      },
      "source": [
        "higgsdata2 = higgsdata2.replace(-999.00, np.nan)"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3nIeKvktv5i",
        "outputId": "ea91240b-5c3f-4703-c7f5-44822a91c81c"
      },
      "source": [
        "label = np.array(higgsdata2.Label)\n",
        "label"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['s', 'b', 'b', ..., 's', 'b', 'b'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef13oz5utv5j",
        "outputId": "2ebe23c5-2461-415b-e2dd-a8cd08c87fce"
      },
      "source": [
        "weight = np.array(higgsdata2.Weight)\n",
        "weight"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00265331, 2.23358449, 2.34738894, ..., 0.01863612, 1.68161144,\n",
              "       1.87747381])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU_TxRMqtv5l"
      },
      "source": [
        "higgsdata2 = higgsdata2.drop(labels=['Label'], axis=1)\n",
        "higgsdata2 = higgsdata2.drop(labels=['Weight'], axis=1)"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kAQ23hjtDtv"
      },
      "source": [
        "#Adesso, prima di creare il train_test_split dobbiamo ricordarci di standardizzare i dati con sklearn.preprocessing.\n",
        "from sklearn import preprocessing\n",
        "\n",
        "higgsdata2_scaled = pd.DataFrame(sklearn.preprocessing.scale(higgsdata2[higgsdata2.columns],\n",
        "                                                            axis=0), columns = higgsdata2.columns)\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "0U7LJe7WoY-q",
        "outputId": "4bbb2c10-c5fa-4f2a-8505-9ebbdc7c9e1a"
      },
      "source": [
        "higgsdata2_scaled"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EventId</th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.732044</td>\n",
              "      <td>0.289914</td>\n",
              "      <td>0.068332</td>\n",
              "      <td>0.407680</td>\n",
              "      <td>-0.469966</td>\n",
              "      <td>-0.857377</td>\n",
              "      <td>-0.621258</td>\n",
              "      <td>0.973036</td>\n",
              "      <td>0.882478</td>\n",
              "      <td>1.033099</td>\n",
              "      <td>0.339894</td>\n",
              "      <td>0.170929</td>\n",
              "      <td>1.277084</td>\n",
              "      <td>-0.647865</td>\n",
              "      <td>-0.270811</td>\n",
              "      <td>0.846712</td>\n",
              "      <td>0.214212</td>\n",
              "      <td>0.225054</td>\n",
              "      <td>1.812288</td>\n",
              "      <td>-1.352820</td>\n",
              "      <td>-0.756757</td>\n",
              "      <td>-0.147267</td>\n",
              "      <td>0.386847</td>\n",
              "      <td>1.044402</td>\n",
              "      <td>-0.286622</td>\n",
              "      <td>1.206627</td>\n",
              "      <td>0.251681</td>\n",
              "      <td>-0.36321</td>\n",
              "      <td>0.616148</td>\n",
              "      <td>-1.361312</td>\n",
              "      <td>0.412510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.732030</td>\n",
              "      <td>0.682021</td>\n",
              "      <td>0.552505</td>\n",
              "      <td>0.540136</td>\n",
              "      <td>-0.153167</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.404888</td>\n",
              "      <td>-0.756027</td>\n",
              "      <td>-0.287584</td>\n",
              "      <td>-0.661279</td>\n",
              "      <td>1.292164</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.147536</td>\n",
              "      <td>1.688504</td>\n",
              "      <td>-1.652849</td>\n",
              "      <td>-0.441526</td>\n",
              "      <td>0.411475</td>\n",
              "      <td>0.032730</td>\n",
              "      <td>0.090798</td>\n",
              "      <td>-1.051683</td>\n",
              "      <td>-0.357719</td>\n",
              "      <td>0.021305</td>\n",
              "      <td>-0.636248</td>\n",
              "      <td>0.408102</td>\n",
              "      <td>0.645421</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.273820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.732016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.195156</td>\n",
              "      <td>1.096560</td>\n",
              "      <td>-0.349710</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.989770</td>\n",
              "      <td>-0.430168</td>\n",
              "      <td>0.340361</td>\n",
              "      <td>2.768174</td>\n",
              "      <td>1.292164</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.292406</td>\n",
              "      <td>-0.571650</td>\n",
              "      <td>-1.147554</td>\n",
              "      <td>3.387682</td>\n",
              "      <td>-0.737951</td>\n",
              "      <td>0.555132</td>\n",
              "      <td>0.382001</td>\n",
              "      <td>-1.200672</td>\n",
              "      <td>0.400135</td>\n",
              "      <td>0.021305</td>\n",
              "      <td>-0.668805</td>\n",
              "      <td>1.152271</td>\n",
              "      <td>-1.111520</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.293970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.732002</td>\n",
              "      <td>0.384768</td>\n",
              "      <td>0.910379</td>\n",
              "      <td>-0.005853</td>\n",
              "      <td>-0.903016</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.196690</td>\n",
              "      <td>-0.830735</td>\n",
              "      <td>-0.712705</td>\n",
              "      <td>1.084818</td>\n",
              "      <td>-0.969095</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.716598</td>\n",
              "      <td>-1.354138</td>\n",
              "      <td>0.010002</td>\n",
              "      <td>0.301873</td>\n",
              "      <td>-0.397234</td>\n",
              "      <td>-1.730447</td>\n",
              "      <td>-0.323312</td>\n",
              "      <td>0.038692</td>\n",
              "      <td>-0.978149</td>\n",
              "      <td>-1.001792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.745439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.731988</td>\n",
              "      <td>0.942536</td>\n",
              "      <td>-0.914556</td>\n",
              "      <td>1.313369</td>\n",
              "      <td>-0.651804</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.938794</td>\n",
              "      <td>-0.112795</td>\n",
              "      <td>-0.868143</td>\n",
              "      <td>-0.451747</td>\n",
              "      <td>-1.052877</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.468428</td>\n",
              "      <td>-1.800568</td>\n",
              "      <td>-1.223513</td>\n",
              "      <td>-0.765298</td>\n",
              "      <td>0.646261</td>\n",
              "      <td>0.839728</td>\n",
              "      <td>-1.185429</td>\n",
              "      <td>-0.475042</td>\n",
              "      <td>-1.238475</td>\n",
              "      <td>-1.001792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.745439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249995</th>\n",
              "      <td>1.731988</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.643636</td>\n",
              "      <td>-1.093204</td>\n",
              "      <td>-0.830312</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-1.253146</td>\n",
              "      <td>-0.622954</td>\n",
              "      <td>-0.886214</td>\n",
              "      <td>-0.212621</td>\n",
              "      <td>-1.077173</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.622586</td>\n",
              "      <td>-0.331962</td>\n",
              "      <td>-0.493642</td>\n",
              "      <td>-0.703526</td>\n",
              "      <td>-0.735579</td>\n",
              "      <td>0.185212</td>\n",
              "      <td>0.146005</td>\n",
              "      <td>1.583208</td>\n",
              "      <td>-0.514882</td>\n",
              "      <td>-1.001792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.745439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249996</th>\n",
              "      <td>1.732002</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.252913</td>\n",
              "      <td>-0.320829</td>\n",
              "      <td>-0.557013</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.270657</td>\n",
              "      <td>0.158111</td>\n",
              "      <td>-0.931795</td>\n",
              "      <td>-0.326265</td>\n",
              "      <td>-1.019364</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.682286</td>\n",
              "      <td>-1.316249</td>\n",
              "      <td>1.532493</td>\n",
              "      <td>-0.881863</td>\n",
              "      <td>0.258903</td>\n",
              "      <td>0.549627</td>\n",
              "      <td>0.152601</td>\n",
              "      <td>-0.472835</td>\n",
              "      <td>-1.022845</td>\n",
              "      <td>-1.001792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.745439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249997</th>\n",
              "      <td>1.732016</td>\n",
              "      <td>-0.286249</td>\n",
              "      <td>0.319316</td>\n",
              "      <td>-0.130864</td>\n",
              "      <td>-0.284955</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.021586</td>\n",
              "      <td>0.146617</td>\n",
              "      <td>-0.328162</td>\n",
              "      <td>-0.278913</td>\n",
              "      <td>0.550699</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.137043</td>\n",
              "      <td>-0.210058</td>\n",
              "      <td>-1.719451</td>\n",
              "      <td>-0.173407</td>\n",
              "      <td>0.316612</td>\n",
              "      <td>0.444486</td>\n",
              "      <td>-0.556268</td>\n",
              "      <td>-1.589146</td>\n",
              "      <td>-0.086089</td>\n",
              "      <td>0.021305</td>\n",
              "      <td>-0.706044</td>\n",
              "      <td>1.010498</td>\n",
              "      <td>-0.084708</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.317017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249998</th>\n",
              "      <td>1.732030</td>\n",
              "      <td>-0.469607</td>\n",
              "      <td>-0.845324</td>\n",
              "      <td>-0.302973</td>\n",
              "      <td>-0.697378</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.266941</td>\n",
              "      <td>-0.243040</td>\n",
              "      <td>-0.886500</td>\n",
              "      <td>-0.519223</td>\n",
              "      <td>1.292164</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.480252</td>\n",
              "      <td>-1.812100</td>\n",
              "      <td>1.541300</td>\n",
              "      <td>-0.849550</td>\n",
              "      <td>-0.675499</td>\n",
              "      <td>-0.186910</td>\n",
              "      <td>-0.898847</td>\n",
              "      <td>0.453102</td>\n",
              "      <td>-0.767429</td>\n",
              "      <td>-1.001792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.745439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249999</th>\n",
              "      <td>1.732044</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.665336</td>\n",
              "      <td>-0.253523</td>\n",
              "      <td>-0.792028</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.444623</td>\n",
              "      <td>-0.513541</td>\n",
              "      <td>-0.649856</td>\n",
              "      <td>-0.593802</td>\n",
              "      <td>-1.074660</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.191664</td>\n",
              "      <td>1.396925</td>\n",
              "      <td>1.464790</td>\n",
              "      <td>-0.291151</td>\n",
              "      <td>1.193306</td>\n",
              "      <td>0.326684</td>\n",
              "      <td>-0.030042</td>\n",
              "      <td>-0.875104</td>\n",
              "      <td>-0.872671</td>\n",
              "      <td>-1.001792</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.745439</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250000 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         EventId  DER_mass_MMC  ...  PRI_jet_subleading_phi  PRI_jet_all_pt\n",
              "0      -1.732044      0.289914  ...               -1.361312        0.412510\n",
              "1      -1.732030      0.682021  ...                     NaN       -0.273820\n",
              "2      -1.732016           NaN  ...                     NaN       -0.293970\n",
              "3      -1.732002      0.384768  ...                     NaN       -0.745439\n",
              "4      -1.731988      0.942536  ...                     NaN       -0.745439\n",
              "...          ...           ...  ...                     ...             ...\n",
              "249995  1.731988           NaN  ...                     NaN       -0.745439\n",
              "249996  1.732002           NaN  ...                     NaN       -0.745439\n",
              "249997  1.732016     -0.286249  ...                     NaN       -0.317017\n",
              "249998  1.732030     -0.469607  ...                     NaN       -0.745439\n",
              "249999  1.732044           NaN  ...                     NaN       -0.745439\n",
              "\n",
              "[250000 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dk2hmSmoY8D"
      },
      "source": [
        "#ci vuole troppo tempo\n",
        "#from sklearn.impute import KNNImputer\n",
        "\n",
        "#make sure to scale before kNN and make sure to use the sclaed data as input of\n",
        "#imputer\n",
        "\n",
        "#imputer = KNNImputer(n_neighbors=2)\n",
        "#higgsdata2_imputed = imputer.fit_transform(higgsdata2_scaled)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU1pPJP1-IPf"
      },
      "source": [
        "Torniamo all'esercizio, tralasciando il KNN, e vediamo come si comporta il random forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCQskgt8oY4q",
        "outputId": "17237437-0a08-47bd-bd01-0e186cf78856"
      },
      "source": [
        "rf.fit(X_traindata, y_trainlabel)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDbGE6G7lC4o",
        "outputId": "66cb3979-5d54-4d02-96bc-04409e4d9406"
      },
      "source": [
        "#accuratezza del classifier           #accuratezza del classfier sul test set\n",
        "rf.score(X_traindata, y_trainlabel), rf.score(X_testdata, y_testlabel)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9999804247822257, 0.8443243878090316)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4L9fZZSCy4V"
      },
      "source": [
        "###**Un'accuratezza così elevata, ed uno score molto più basso nel test, significa che c'è stato overfitting. Infatti dovevo controllare meglio gli hyperparameters per avere uno score decente sia per il training che per il test, cambiando la depth, il numero di foglie, ecc..**\n",
        "\n",
        "\n",
        "##**HO DIMENTICATO DI FARE LE CONFUSION MATRIX ANCHE...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVxn4aOElCwX"
      },
      "source": [
        "##Perchè non c'è bisogno di scalare i dati quando usiamo i modelli con alberi? \n",
        "\n",
        "**Perchè i tree models, rispetto agli altri modelli, trattano una feature alla volta. Allora non ha importanza il range, che siano comparabili, ecc.. perchè la scelta è indipendente per il mio x-axis e per il y-axis.**\n",
        "\n",
        "Ad ogni nodo dell'albero la scelta è basata solo su quella variabile: la variabile è automaticamente scalata a se stessa quando faccio la scelta. Lo stesso motivo per cui non mi preoccupo della covariance. \n",
        "\n",
        "Quando faccio una scelta, la scelta successiva non considera la variabile precedente perchè è già stata usata (ecco perchè con varibili che hanno covariance, vanno bene i tree models).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUyNAZ9yAYze"
      },
      "source": [
        "Guardando gli hist delle variabili fatte dalla prof, noto che alcune delle variabili non sono gaussiane. Non è un proobelma per trees ma è un grande problema per linear regression method, ad esempio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEx0-vdS_ofI"
      },
      "source": [
        "La prof per pulire dai NaN, ha tolto 3 colonne: PRI_jet_subleading_pt, \tPRI_jet_subleading_eta, ed\tPRI_jet_subleading_phi che contenevano molti NaN. E poi ha rimosso tutte le righe che hanno NaN sulle altre osservaizoni."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3srFvzzDheO"
      },
      "source": [
        "#CONTINUIAMO CON LA PROF CON LA TEORIA\n",
        "\n",
        "Come si può valutare il risulatato del modello in maniera più sofisticata rispetto allo score. Lo score non mi dice cosa ho sbagliato. Cioè potrei sbagliare la predizione tra b ed s, cioè tra background e signal, e questa è una cosa importante perchè predirrei un background quando invece era un signal e quindi mi sono perso una particella, missing discovery! **(False Positive and False Negative, che non sono la stessa cosa)**\n",
        "\n",
        "**Il modo di capire se ho più false positive o false negative è costruire una confusion matrix.** Il fatto che ho pochi off diagonal e sono abbastanza bilanciati, a livello di qeusta scala ho un numero decente di false positive e false negative. \n",
        "\n",
        "**Il gbt produce un risultato migliore rispetto al rf, perchè l'overprediction del training del rf mi espone all apossibilità in un diverso tsting data potrei ottenere uno score ancora peggiore.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAe4tCEVKN_s"
      },
      "source": [
        "##Possiamo esercitarci usando il random forest regressor per predire i pesi delle particelle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hquH-kwOKf80",
        "outputId": "041293b7-bbee-4291-de83-34eb766fc438"
      },
      "source": [
        "weight.shape"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89TsCUyuKgG7"
      },
      "source": [
        "#Adesso, prima di creare il train_test_split dobbiamo ricordarci di standardizzare i dati con sklearn.preprocessing.\n",
        "from sklearn import preprocessing\n",
        "\n",
        "higgsdata_scaled = pd.DataFrame(sklearn.preprocessing.scale(higgsdata[higgsdata.columns],\n",
        "                                                            axis=0), columns = higgsdata.columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "0iGF7kqdKrVM",
        "outputId": "c85521d9-4122-4d38-92d2-0150c7fedd32"
      },
      "source": [
        "higgsdata"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DER_mass_MMC</th>\n",
              "      <th>DER_mass_transverse_met_lep</th>\n",
              "      <th>DER_mass_vis</th>\n",
              "      <th>DER_pt_h</th>\n",
              "      <th>DER_deltaeta_jet_jet</th>\n",
              "      <th>DER_mass_jet_jet</th>\n",
              "      <th>DER_prodeta_jet_jet</th>\n",
              "      <th>DER_deltar_tau_lep</th>\n",
              "      <th>DER_pt_tot</th>\n",
              "      <th>DER_sum_pt</th>\n",
              "      <th>DER_pt_ratio_lep_tau</th>\n",
              "      <th>DER_met_phi_centrality</th>\n",
              "      <th>DER_lep_eta_centrality</th>\n",
              "      <th>PRI_tau_pt</th>\n",
              "      <th>PRI_tau_eta</th>\n",
              "      <th>PRI_tau_phi</th>\n",
              "      <th>PRI_lep_pt</th>\n",
              "      <th>PRI_lep_eta</th>\n",
              "      <th>PRI_lep_phi</th>\n",
              "      <th>PRI_met</th>\n",
              "      <th>PRI_met_phi</th>\n",
              "      <th>PRI_met_sumet</th>\n",
              "      <th>PRI_jet_num</th>\n",
              "      <th>PRI_jet_leading_pt</th>\n",
              "      <th>PRI_jet_leading_eta</th>\n",
              "      <th>PRI_jet_leading_phi</th>\n",
              "      <th>PRI_jet_subleading_pt</th>\n",
              "      <th>PRI_jet_subleading_eta</th>\n",
              "      <th>PRI_jet_subleading_phi</th>\n",
              "      <th>PRI_jet_all_pt</th>\n",
              "      <th>Weight</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>138.470</td>\n",
              "      <td>51.655</td>\n",
              "      <td>97.827</td>\n",
              "      <td>27.980</td>\n",
              "      <td>0.910</td>\n",
              "      <td>124.711</td>\n",
              "      <td>2.666</td>\n",
              "      <td>3.064</td>\n",
              "      <td>41.928</td>\n",
              "      <td>197.760</td>\n",
              "      <td>1.582</td>\n",
              "      <td>1.396</td>\n",
              "      <td>0.200</td>\n",
              "      <td>32.638</td>\n",
              "      <td>1.017</td>\n",
              "      <td>0.381</td>\n",
              "      <td>51.626</td>\n",
              "      <td>2.273</td>\n",
              "      <td>-2.414</td>\n",
              "      <td>16.824</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>258.733</td>\n",
              "      <td>2</td>\n",
              "      <td>67.435</td>\n",
              "      <td>2.150</td>\n",
              "      <td>0.444</td>\n",
              "      <td>46.062</td>\n",
              "      <td>1.240</td>\n",
              "      <td>-2.475</td>\n",
              "      <td>113.497</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>89.744</td>\n",
              "      <td>13.550</td>\n",
              "      <td>59.149</td>\n",
              "      <td>116.344</td>\n",
              "      <td>2.636</td>\n",
              "      <td>284.584</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.362</td>\n",
              "      <td>61.619</td>\n",
              "      <td>278.876</td>\n",
              "      <td>0.588</td>\n",
              "      <td>0.479</td>\n",
              "      <td>0.975</td>\n",
              "      <td>53.651</td>\n",
              "      <td>0.371</td>\n",
              "      <td>1.329</td>\n",
              "      <td>31.565</td>\n",
              "      <td>-0.884</td>\n",
              "      <td>1.857</td>\n",
              "      <td>40.735</td>\n",
              "      <td>2.237</td>\n",
              "      <td>282.849</td>\n",
              "      <td>3</td>\n",
              "      <td>90.547</td>\n",
              "      <td>-2.412</td>\n",
              "      <td>-0.653</td>\n",
              "      <td>56.165</td>\n",
              "      <td>0.224</td>\n",
              "      <td>3.106</td>\n",
              "      <td>193.660</td>\n",
              "      <td>0.083414</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>148.754</td>\n",
              "      <td>28.862</td>\n",
              "      <td>107.782</td>\n",
              "      <td>106.130</td>\n",
              "      <td>0.733</td>\n",
              "      <td>158.359</td>\n",
              "      <td>0.113</td>\n",
              "      <td>2.941</td>\n",
              "      <td>2.545</td>\n",
              "      <td>305.967</td>\n",
              "      <td>3.371</td>\n",
              "      <td>1.393</td>\n",
              "      <td>0.791</td>\n",
              "      <td>28.850</td>\n",
              "      <td>1.113</td>\n",
              "      <td>2.409</td>\n",
              "      <td>97.240</td>\n",
              "      <td>0.675</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>38.421</td>\n",
              "      <td>-1.443</td>\n",
              "      <td>294.074</td>\n",
              "      <td>2</td>\n",
              "      <td>123.010</td>\n",
              "      <td>0.864</td>\n",
              "      <td>1.450</td>\n",
              "      <td>56.867</td>\n",
              "      <td>0.131</td>\n",
              "      <td>-2.767</td>\n",
              "      <td>179.877</td>\n",
              "      <td>0.002653</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>114.744</td>\n",
              "      <td>10.286</td>\n",
              "      <td>75.712</td>\n",
              "      <td>30.816</td>\n",
              "      <td>2.563</td>\n",
              "      <td>252.599</td>\n",
              "      <td>-1.401</td>\n",
              "      <td>2.888</td>\n",
              "      <td>36.745</td>\n",
              "      <td>239.804</td>\n",
              "      <td>1.061</td>\n",
              "      <td>1.364</td>\n",
              "      <td>0.769</td>\n",
              "      <td>35.976</td>\n",
              "      <td>-0.669</td>\n",
              "      <td>-0.342</td>\n",
              "      <td>38.188</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>2.502</td>\n",
              "      <td>22.385</td>\n",
              "      <td>2.148</td>\n",
              "      <td>290.547</td>\n",
              "      <td>3</td>\n",
              "      <td>76.773</td>\n",
              "      <td>-0.790</td>\n",
              "      <td>0.303</td>\n",
              "      <td>56.876</td>\n",
              "      <td>1.773</td>\n",
              "      <td>-2.079</td>\n",
              "      <td>165.640</td>\n",
              "      <td>0.307170</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>141.481</td>\n",
              "      <td>0.736</td>\n",
              "      <td>111.581</td>\n",
              "      <td>174.075</td>\n",
              "      <td>1.955</td>\n",
              "      <td>364.344</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>1.335</td>\n",
              "      <td>6.663</td>\n",
              "      <td>440.859</td>\n",
              "      <td>0.652</td>\n",
              "      <td>1.042</td>\n",
              "      <td>0.207</td>\n",
              "      <td>98.565</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-1.506</td>\n",
              "      <td>64.285</td>\n",
              "      <td>1.405</td>\n",
              "      <td>-0.952</td>\n",
              "      <td>17.960</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>454.785</td>\n",
              "      <td>2</td>\n",
              "      <td>195.533</td>\n",
              "      <td>1.156</td>\n",
              "      <td>1.416</td>\n",
              "      <td>82.477</td>\n",
              "      <td>-0.798</td>\n",
              "      <td>-2.785</td>\n",
              "      <td>278.009</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249976</th>\n",
              "      <td>137.371</td>\n",
              "      <td>4.640</td>\n",
              "      <td>92.603</td>\n",
              "      <td>107.121</td>\n",
              "      <td>3.189</td>\n",
              "      <td>322.430</td>\n",
              "      <td>-2.384</td>\n",
              "      <td>2.149</td>\n",
              "      <td>2.755</td>\n",
              "      <td>225.261</td>\n",
              "      <td>1.783</td>\n",
              "      <td>1.090</td>\n",
              "      <td>0.503</td>\n",
              "      <td>32.004</td>\n",
              "      <td>0.599</td>\n",
              "      <td>-0.013</td>\n",
              "      <td>57.056</td>\n",
              "      <td>-0.924</td>\n",
              "      <td>1.503</td>\n",
              "      <td>42.299</td>\n",
              "      <td>1.408</td>\n",
              "      <td>228.186</td>\n",
              "      <td>2</td>\n",
              "      <td>85.132</td>\n",
              "      <td>1.991</td>\n",
              "      <td>-1.518</td>\n",
              "      <td>51.068</td>\n",
              "      <td>-1.197</td>\n",
              "      <td>-2.831</td>\n",
              "      <td>136.200</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249980</th>\n",
              "      <td>119.934</td>\n",
              "      <td>20.078</td>\n",
              "      <td>88.751</td>\n",
              "      <td>35.227</td>\n",
              "      <td>0.660</td>\n",
              "      <td>111.491</td>\n",
              "      <td>1.836</td>\n",
              "      <td>2.800</td>\n",
              "      <td>18.532</td>\n",
              "      <td>189.198</td>\n",
              "      <td>1.951</td>\n",
              "      <td>0.304</td>\n",
              "      <td>0.000</td>\n",
              "      <td>25.844</td>\n",
              "      <td>1.159</td>\n",
              "      <td>0.823</td>\n",
              "      <td>50.416</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-1.558</td>\n",
              "      <td>2.113</td>\n",
              "      <td>1.116</td>\n",
              "      <td>237.326</td>\n",
              "      <td>2</td>\n",
              "      <td>69.219</td>\n",
              "      <td>-1.064</td>\n",
              "      <td>1.118</td>\n",
              "      <td>43.719</td>\n",
              "      <td>-1.725</td>\n",
              "      <td>-2.756</td>\n",
              "      <td>112.938</td>\n",
              "      <td>0.018636</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249985</th>\n",
              "      <td>126.151</td>\n",
              "      <td>29.023</td>\n",
              "      <td>95.258</td>\n",
              "      <td>152.684</td>\n",
              "      <td>1.000</td>\n",
              "      <td>163.066</td>\n",
              "      <td>-0.240</td>\n",
              "      <td>1.504</td>\n",
              "      <td>24.642</td>\n",
              "      <td>327.502</td>\n",
              "      <td>2.163</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.027</td>\n",
              "      <td>42.495</td>\n",
              "      <td>1.980</td>\n",
              "      <td>2.239</td>\n",
              "      <td>91.908</td>\n",
              "      <td>0.851</td>\n",
              "      <td>-3.050</td>\n",
              "      <td>41.247</td>\n",
              "      <td>-2.575</td>\n",
              "      <td>402.114</td>\n",
              "      <td>2</td>\n",
              "      <td>158.904</td>\n",
              "      <td>0.401</td>\n",
              "      <td>0.034</td>\n",
              "      <td>34.196</td>\n",
              "      <td>-0.599</td>\n",
              "      <td>-2.525</td>\n",
              "      <td>193.099</td>\n",
              "      <td>0.018636</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249993</th>\n",
              "      <td>130.075</td>\n",
              "      <td>3.918</td>\n",
              "      <td>66.781</td>\n",
              "      <td>77.369</td>\n",
              "      <td>0.936</td>\n",
              "      <td>322.296</td>\n",
              "      <td>-0.207</td>\n",
              "      <td>3.102</td>\n",
              "      <td>49.937</td>\n",
              "      <td>610.482</td>\n",
              "      <td>1.354</td>\n",
              "      <td>-0.634</td>\n",
              "      <td>0.000</td>\n",
              "      <td>27.364</td>\n",
              "      <td>2.403</td>\n",
              "      <td>1.348</td>\n",
              "      <td>37.052</td>\n",
              "      <td>1.775</td>\n",
              "      <td>-1.689</td>\n",
              "      <td>67.702</td>\n",
              "      <td>-1.768</td>\n",
              "      <td>694.010</td>\n",
              "      <td>3</td>\n",
              "      <td>155.864</td>\n",
              "      <td>-0.358</td>\n",
              "      <td>1.093</td>\n",
              "      <td>134.344</td>\n",
              "      <td>0.578</td>\n",
              "      <td>-2.215</td>\n",
              "      <td>546.066</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>s</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249994</th>\n",
              "      <td>217.020</td>\n",
              "      <td>47.156</td>\n",
              "      <td>62.824</td>\n",
              "      <td>127.953</td>\n",
              "      <td>0.295</td>\n",
              "      <td>119.437</td>\n",
              "      <td>-0.014</td>\n",
              "      <td>2.318</td>\n",
              "      <td>3.628</td>\n",
              "      <td>242.586</td>\n",
              "      <td>1.393</td>\n",
              "      <td>1.393</td>\n",
              "      <td>0.000</td>\n",
              "      <td>28.586</td>\n",
              "      <td>1.094</td>\n",
              "      <td>1.729</td>\n",
              "      <td>39.824</td>\n",
              "      <td>0.700</td>\n",
              "      <td>-0.555</td>\n",
              "      <td>97.737</td>\n",
              "      <td>0.220</td>\n",
              "      <td>271.082</td>\n",
              "      <td>2</td>\n",
              "      <td>141.752</td>\n",
              "      <td>0.237</td>\n",
              "      <td>3.126</td>\n",
              "      <td>32.423</td>\n",
              "      <td>-0.058</td>\n",
              "      <td>-1.137</td>\n",
              "      <td>174.176</td>\n",
              "      <td>0.064061</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>68114 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        DER_mass_MMC  DER_mass_transverse_met_lep  ...    Weight  Label\n",
              "0            138.470                       51.655  ...  0.002653      s\n",
              "5             89.744                       13.550  ...  0.083414      b\n",
              "6            148.754                       28.862  ...  0.002653      s\n",
              "11           114.744                       10.286  ...  0.307170      b\n",
              "23           141.481                        0.736  ...  0.001503      s\n",
              "...              ...                          ...  ...       ...    ...\n",
              "249976       137.371                        4.640  ...  0.001503      s\n",
              "249980       119.934                       20.078  ...  0.018636      s\n",
              "249985       126.151                       29.023  ...  0.018636      s\n",
              "249993       130.075                        3.918  ...  0.001503      s\n",
              "249994       217.020                       47.156  ...  0.064061      b\n",
              "\n",
              "[68114 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "552mi9KrKgG-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Per splittare i dati va anche bene 75% e 25% se sono molto grandi. \n",
        "#Posso anche usare i dati non scalati dal momento che uso un tree model.\n",
        "\n",
        "X_traindata2, X_testdata2, y_train_weight2, y_test_weight2 = train_test_split(higgsdata[higgsdata.columns], \n",
        "                    weight, train_size=0.75, test_size=0.25, random_state=1) "
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaZnl3Dk_n3I",
        "outputId": "c64300b1-48b1-428f-e773-6240c4310aa8"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf2 = RandomForestRegressor(n_estimators=100, max_depth=3,\n",
        "                            random_state=1)\n",
        "rf2.fit(X_traindata2, y_train_weight2)\n",
        "print(rf2.feature_importances_)"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.00295196 0.32764663 0.32002897 0.         0.         0.23402424\n",
            " 0.         0.00541278 0.0023263  0.00238757 0.         0.01995291\n",
            " 0.         0.07103609 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00205537 0.01149645 0.\n",
            " 0.         0.         0.         0.         0.         0.00068074]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIRwxCQSMu31",
        "outputId": "b50381b5-abdf-48ff-a17d-8f58a8f8ac27"
      },
      "source": [
        "rf2.predict(X_testdata2)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.44610028, 0.12222057, 0.46078605, ..., 0.12222057, 0.45325785,\n",
              "       0.12222057])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krutcVikJY3T"
      },
      "source": [
        "In questo caso non ho più l'accuratezza, ma posso vedere qualcosa relazionato al chisq o ad L2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpuv0VApD-Bk",
        "outputId": "1edc755a-4d77-4967-b0e3-f74e35c5061b"
      },
      "source": [
        "print(\"L2 RF\", ((rf2.predict(X_testdata2) - y_test_weight2.flatten())**2).sum() )"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 RF 6853.054435715192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNfs497ENk7X"
      },
      "source": [
        "Questo stampa solo L2, ma se voglio vedere qunato ciascuna predizione è sbagliata dei dividere per il num di osservazionin che ho per ottenere l'average errore prediciton. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVZvQITgNz7L"
      },
      "source": [
        "#print(\"L2 RF\", ((np.abs(gbt2.predict(X_testdata2) - y_test_weight2.flatten())**2).sum() )"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwUT9ukzN9iF"
      },
      "source": [
        "#RandomizedsearchCV\n",
        "\n",
        "Ovvero una gridsearch sugli stimatori che avrebbe portato via molto tempo, quindi non lo facciamo. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzj81PsjO2KK"
      },
      "source": [
        "#A parte lo score e la metrica per la regression della chisq.. : \n",
        "\n",
        "il modo più comune e preferito epr valutare la performance di un classifier probabilistic è la ROC (Receiver Operator caratteristic curve) e la EOC che è l'area all'interno della curva del grafico sotto.\n",
        "\n",
        "ROC mi dice qual è la balance tra fasle positive e false negative sulla base della threshold nella probabilità della classificazione. \n",
        "\n",
        "Nel random forest ha una classificazione probab intuitiva: se ho 1000 alberi e alcuni predicono signal e altri backgroud. Se metà dicono s e metà dicono b, dal punto di vista frequentistico ho il 50% di probabilità per un evento di essere signal.\n",
        "\n",
        "Adesso devo decidere la threshold per scegliere quale è signal e quale background. \n",
        "Devo scegliere la confidenza ad es del 90% (cioè il 90% degli alberi devono essere d'accordo nel dare una classificazione). \n",
        "\n",
        "Come funzoin di questa threshold posso fare un punto sul plot della ROC. Sulla base della threshold ho i punti arancioni sotto, per ongi threshold di acceptance of true postive ho un punto nel grafico. \n",
        "\n",
        "In questo grafico, nel modello ideale vorrei avere 0 false positive e 100% true positive.\n",
        "Ma siccome non accade mai, il punto che scelgo dipende dalla threshold, ovvero dall'interesse scientifico: se posso tollerare false positive o no. \n",
        "\n",
        "Se invece non posso tollerare false positive (ad esempio in prosecutorial justice) voglio stare nella parte a sx della curva, ovvero per un numero molto basso di false positive, ma avrò anche un numero inferiore di false negative. Le performance totale del modello è misurata dell'area sotto la curva: la EOC.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9lpD21gQmGz"
      },
      "source": [
        "trasformo y_testlabel in un dataframe perchè altrimenti non mi fa il plot della roc curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnbIZEoqQFhs",
        "outputId": "185ca15a-ccb5-404d-8d0d-772939d41af1"
      },
      "source": [
        "pd.DataFrame(y_testlabel).values"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['b'],\n",
              "       ['s'],\n",
              "       ['b'],\n",
              "       ...,\n",
              "       ['b'],\n",
              "       ['s'],\n",
              "       ['b']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Sh0KkCbDORe8",
        "outputId": "d6736cf0-6940-4c9b-9033-7c2c7d97841a"
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_grd_rfcat = rf.predict_proba(X_testdata)[:, 1]\n",
        "\n",
        "#la prof ha usato il comando commentato e funzionava... aveva testlabel che\n",
        "#era già un dataframe\n",
        "#fpr_rf, tpr_rf, _ = roc_curve(y_testlabel.values.flatten()==\"s\", y_pred_grd_rfcat)\n",
        "\n",
        "#NOTA CHE ROC_CURVE VUOLE IN INPUT NELLA X UNA BOOLEAN VALUE\n",
        "fpr_rf, tpr_rf, _ = roc_curve(pd.DataFrame(y_testlabel).values.flatten()==\"s\", y_pred_grd_rfcat)\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr_rf, tpr_rf, label='RF')\n",
        "plt.plot(fpr_rf, tpr_rf, '.', label='RF')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfbA8e+ZmYSiiDRRJBA6BETECCIGRRRFReyiiKggAupa17Uisv6wASoICiqCiL0suLLWtbAohipSRBACA4J0kJZMOb8/7k2IMSEDZDKZzPk8zzyZcmfm3ATec98uqooxxpjE5Yl1AMYYY2LLEoExxiQ4SwTGGJPgLBEYY0yCs0RgjDEJzhKBMcYkOEsExhiT4CwRmHJHRLJEZK+I7BKRDSIyUUSOLHDMaSLyXxH5Q0R2iMhHIpJW4JijRORZEVnjftav7uOapXtGxkSXJQJTXnVX1SOBNsBJwP25L4hIB+AzYCpQB2gA/AjMFJGG7jHJwJdAS+A84CigA7AFaBetoEXEF63PNqYolghMuaaqG4BPcRJCrqeA11T1OVX9Q1W3qupDwCxgiHvMdUA94BJVXaKqYVXdqKr/VNXphX2XiLQUkc9FZKuI/C4iD7jPTxSRx/Idd6aIrM33OEtE/iEiC4Hd7v33Cnz2cyIyyr1fVUReEZH1IrJORB4TEe9h/qpMArNEYMo1EakLdANWuI8rA6cB7xZy+DvAOe79s4FPVHVXhN9TBfgC+ASnltEYp0YRqauBC4CjgbeA893PxC3krwTecI+dCATd7zgJ6Ar0O4jvMuZPLBGY8upfIvIH4Ac2Ao+4z1fH+Xe/vpD3rAdy2/9rFHFMUS4ENqjqCFXd59Y0fjiI949SVb+q7lXV1cA84BL3tbOAPao6S0RqA+cDd6jqblXdCDwD9DyI7zLmTywRmPLqYlWtApwJNGd/Ab8NCAPHFfKe44DN7v0tRRxTlBTg10OK1OEv8PgNnFoCwDXsrw3UB5KA9SKyXUS2A+OAYw7ju02Cs0RgyjVV/QanKWW4+3g38D1wRSGHX8n+5pwvgHNF5IgIv8oPNCzitd1A5XyPjy0s1AKP3wXOdJu2LmF/IvAD2UBNVT3avR2lqi0jjNOYv7BEYBLBs8A5InKi+/g+oI+I/E1EqohINbcztwPwqHvMZJxC930RaS4iHhGpISIPiMj5hXzHv4HjROQOEangfm5797UFOG3+1UXkWOCO4gJW1U3A18CrwCpVXeo+vx5nxNMId3irR0QaicgZh/B7MQawRGASgFuovgYMdh//DzgXuBSnH2A1Tqfr6aq63D0mG6fD+Gfgc2AnkInTxPSXtn9V/QOno7k7sAFYDnR2X56MMzw1C6cQfzvC0N9wY3ijwPPXAcnAEpymrvc4uGYsY/5EbGMaY4xJbFYjMMaYBGeJwBhjEpwlAmOMSXCWCIwxJsHF3QJXNWvW1NTU1FiHYYwxcWXu3LmbVbVWYa/FXSJITU1lzpw5sQ7DGGPiioisLuo1axoyxpgEZ4nAGGMSnCUCY4xJcHHXR1CYQCDA2rVr2bdvX6xDOSQVK1akbt26JCUlxToUY0wCKheJYO3atVSpUoXU1FREJNbhHBRVZcuWLaxdu5YGDRrEOhxjTAKKWtOQiEwQkY0isqiI10VERonIChFZKCJtD/W79u3bR40aNeIuCQCICDVq1Ijb2owxJv5Fs49gIs6m30XpBjRxb/2BFw7ny+IxCeSK59iNMYA/E2aMcH4e7rFFvX4w33GQotY0pKrfikjqAQ7pgbOBuAKzRORoETnOXW/dGGOK5s+ErBmQmgEp7Q7/uALv0VUzCNbrSM5x6QRDSk4oTDAcJhBUAuEwgVA47/mK6+fS7LNeeEI5hL3JzDtjEltrtCEU1r/cjt46n7N+uAlPOIewJ4npbcexvkprQqqEw0qt7Qu5dNFAvOEAIU8S77Ucw/oqram5bT49f/4bSQQRbzL0mRb5+UQgln0Ex/Pn7fnWus/9JRGISH+cWgP16tUrleAOltfr5YQTTiAYDNKgQQMmT57M0UcfTVZWFi1atKBZs2Z5x2ZmZpKcnBzDaI0pAYdRGKsqgZCSHQyxLxAmOxgCfyY+/0y2H9OezUe3YW8gxN5AiH3ubW+Oc2y1rQu4eukteDRASJIYW28ky5PTCITcAjqs5ATDNNy3hCHb7sNHkAA+/l75n/wkzQipEgopwbASVudnKKSEVGkVXsYk72MkESSIj945DzBPmx7w1zDIO5WmvhxEwmgwh68+/YCxoVARx/6bs3w5eCRMOBTg5+//w9hQcr7XP0V8gbzX/fM/54VgZQZ6v8TjCyAShlCO8/ssJ4kgYqo6HhgPkJ6eXiY3UKhUqRILFiwAoE+fPowZM4YHH3wQgEaNGuW9ZkypKK6QLvB6OKzsDYTYkxNiT04QXZNJ8tqZbK7Vji1uobwvr2AOU3XzfHosHIA3HCAoSbzU4FmWJ7cgOxh2byGyA2FS9y7msZ0P4HML1v4ymNnBJmQHQ4Tz/U9uK78wJXkYSQSpio8HDlAA35b0BeIN4CWMaoDKv33PzxXrkuT1kOT14PMKSV4PbUI/4SOIlzAQ5OzKy5Ea7fF5BG++m88jeNyfHdd/TYW1QTyE8RDiHy02s6jhxSR7BZ/7+Unu5/s8QpLPQ/UtSciXU9FwAI83iUsvuooLjkvf/9ki+DwePB6ouOEovO9PRUMBvL4kbrnuem6p2w6ve5x3XU08k6dBKAevL4nQ5k1kjb2Q2afWR7r5QEPgTXb+biUololgHc6G37nqus/FvQ4dOrBw4cJYh2Hi3YEK83yvBeuksys7yB/7guzYG0D9maR9fi0Scpof3m05lmVJLdi5N8COvQGO3bmQh7fel1c4Xx96iFmBxnkfnb9QroaPIYUUyoO8n+RduXo0gKz+H/MqHEcFn4cKSR4q+LxU8Hk41bOEJLcwFoL0PtZP85SzqeDzUDHJ6x7v5cSsH6jws1sAS4hRHXaz/eTTqZjkHFcpyUtF9+ZdVwsmTYVQDj5vMv2v60P/QpOdFya95RSq3mR69LiKHiknFfM7vxQmTYRQDh5vMu0796B9SnGj+bpCykeQNQNJzaDxga7Uq2VAn4/y/nZHFDy2fnvoM43wym/p9dALvPPdNO69916GDBmCd/NPB9/MFaFYJoJpwK0i8hbQHthREv0Dj360mCW/7Tzs4PJLq3MUj3SPbG/wUCjEl19+Sd++ffOe+/XXX2nTpg0AHTt2ZMyYMSUanynjiinQA79+y87a7fm96ols25PD1t05eH+bzTmzb8q74h5ddwSLvc3ZkxMkZfci/rnDucoOqI9eBQrqQd6ptPDl4M1rXviMD3xVOKpSElUrJXFO4Kc/Fc43119P+/rdqJzspXKylzarM/MKZa+EeP603ew8JYOKPu/+Qvn36ninTMsrjAf1uZ5BhRbGHpj0Zl5hfM75l3NOSou/Hnf8hbBifF4BXPekrtQ9vmrhv8+Udk4beXGFYqTHHe57ct9XAsdu2bKF6nVPwZPSjivvasTdz6SQnp5+8N9xkKKWCETkTeBMoKaIrAUeAZIAVPVFYDpwPrAC2APcEK1YSsPevXtp06YN69ato0WLFpxzzjl5r1nTUDlSVKHuPp9d9zR21DiJHXsD7NznXJ23+W8fPOEcQp4kxqc+y3xtwpbdORy7YyEj9w0miSCV8fFQvgJ9kPdjuua74j7q9x/YWCWVykk+2km+q2wJcnfTjSxrcjFVKvo4qlISx/2RhOdzp6nC60vi7hv7cm/99vlirfCnK+XO515K55R8V/x1L4AV4yCUg3iTqdOmK3WOPerPv4cGHUq2MD7YAjjSQvFQCs8oFrhFUVWmTJnC7bffzhNPPMFNN93EJZdcUmrfH81RQ1cX87oCt5T090Z65V7ScvsI9uzZw7nnnsuYMWP429/+FpNYzCEorIDP95zWPYVty/5H1XcvR0JOof5yg+eYG27M0VsWOO3gGiSMjwF/KtCn0iZf5yBZ/2Nd1RRqHJHM+Uf9SoXs/e3RT5y8g21tT6X6EcnU2lEV7zv7r7hv6n0dN+XF5f3TVXbHsy+h45+aL46F4/c3P3gLFmrFFboHU3iXZGEcgwK4LPD7/QwYMIDp06dz6qmn0rFjx1KPIS46i+NJ5cqVGTVqFBdffDGDBg2KdTimoAIFfjAUZtuymVR//3K3TT2Z/7Qdx56cEJf+NBCPBgjioy+DaR34ibvyNbmEVn3Luqp1OTe3HVycAn3wCVvxtzzJuTrfWRHPJ7mdg8nc0ud6bskr0AUmvZ7XHNK0/fmQUsN5rfbpRRfGkRTUxRWqh/u6KRFvvvkmN998M6FQiGeffZZbb70Vr9db6nFYIoiCk046idatW/Pmm2+SkVGyvfvmAAoU8jlZ3/PH0q/5rVo6qyq1RPyZnDevP55wgCBJ/C15CF/sSuVm+Ve+Aj6Hpd9PB0B8AbzijDjpc5yfQL0LkDm5TS7J3NrnBm5Naec2tbyZV6C3yehOm5Q6blBdoPZHh1agH6gwtoK6XKhWrRrt27dn/PjxMV1iRpwWmviRnp6uBTemWbp0KS1aFNIBFUfKwzmUKrfQD9c7nd+Pbs2mJTNI+7w3nnAOAUliVNKN3Jr9CknuGPJeOQ9wqmcpd/nexSdhgnj4pFZfljbux4n8QpfMm/CEA6g3id1XfUClZC++yRc7Y7bzT+Appo8gGiM6TPkRDAZ55plnyMnJyRterqqlsrqAiMxV1fTCXrMagSmbChas/kxyfv2GFZVPYsXGPzhv7s14NUCO+rjFLeTT3LZ4rwa4wDebCjn7hyOO77SPCo374H17f7v7hRddyYUpzYHmkHZs3vC/KrkFeWFX60VdidsVuinGjz/+SN++fZk7dy5XXnllXgIoC0vMWCIwZUO+K/zNu7Op8f4VTqesJDG52kCu2ToWnwZpgI95oQw8PmdCUbIEebjVVrwNL8fzpdMW7/Mmk3ZWb/hkcV5zTc1WXQ7cFFNYQW6FuykB2dnZPPbYYzzxxBNUr16dd999l8suu6xMJIBclghMbPgz2f3L1yyvdCJZm3dz/vwBeVf4n4Uy6Ol12+zDAU7c+c3+zlgJcemJx+NbUiFv1MxJnbo7BXbdAm3xtdMiv6I3JkqWL1/Ok08+yTXXXMPIkSOpUaNGrEP6C0sEJjoKNO3sW/k9WxZ/yVxpyS+/7+JW/11U0CDN8LGowBV+uwY1kN+S8zplTz7vevjkvryr+8qn9IZTehdfyFuhb2Jk165dTJ06lV69etGqVSt+/vlnGjZsGOuwimSJwJQcfyahld+yPlCZY78bgrizYkd6buCO4ARqE+QcfAR9nf90hX9Fm7r4Fu+/wm/a9Sbgpsiu7o0pYz7//HP69+/P6tWradu2LS1atCjTSQAsEZjD4c8ke8U3LEluzZL1O7l80S14NUBtBCGMVxTVAJdUnEuF3fuXLLj0pDqwoELeFX6F9Gsh/doDF/R2dW/KuG3btnHPPfcwYcIEmjZtyjfffBM3IwEtEZSQhFmG2p9JcOW3LNnmo9mPw/CGAzTHx5JwBj6vM+4+jAfxeFFVfN5kmne+Fj5ZlLdkASde49zsCt+UE6FQiI4dO/LLL79w//33M3jwYCpWrBjrsCJmiaCElMtlqPO18+/OCbL5fxOpk/UBoiFaqDhDNUXxSIgrT0rBu2j/VT7nPQF7t1jTjinXNm/eTPXq1fF6vQwbNox69erRtu0h77obM4mbCKI4ASiul6HOXTwt6Wh8nz+AhHII4sGnUJcQHlEECItz1Y8qHm8ynpN7wcm9Dm2WrDFxRlWZPHkyd9xxB0888QT9+/fn4osvjnVYhywxE4E/EyZd9NdZoyUgXpeh1jU/sPX7SVT9+R1EQ3hUgDAeUbwoHgFBceahCx5fhb9e9YMV9qbcW716NTfffDOffvopp512Gp06dYp1SIctMRNB1gwnCWioxLZ9i8tlqP2Z7Fr2FT9sgI4rhnO0BvCgiADiQcSLong9XkAgHHRqASddCydebYW+STivv/46AwcORFUZPXo0gwYNwuPxxDqsw5aYiSA1w6kJ5NYISmDbt3hahnrrzzPY9L9JNFz7LypqiE7sb+/PveL3FrziB1tLxyS8WrVq0bFjR8aNG0f9+vVjHU6JScxEcKi7EEWgrC5DrWt+YPW8z5ixNsTlm8bQmIDT3i+g7G/vP+AVvyUAk2ACgQAjRowgEAjw8MMPc+6559K1a9cytTxESUjMRABR7bwsS8tQ71r+HWu+foXG66ZSV0P0FHHXzc9ddVaQotr7jUlg8+fPp2/fvsyfP5+ePXuWqUXiSlriJoIStmvXrj89/uijj/LuL1q0qLTDQdf8wK9fvkxK1gc0yx3tU+DqH2vvN+Yv9u3bx9ChQ3nqqaeoWbMm77//Ppdeemmsw4oqSwTljT+Trd9NosrPb9MgHMwb7umwq39jirNixQqGDx/Oddddx4gRI6hWrVqsQ4o6SwTlgT8TXTWDX3cnk/LDP6mqOXmjf3I7f/Em2dW/MUXYtWsXH374Ib1796ZVq1YsW7YspjuGlbZykwhKa5efaDjkXeL8mYTmvwELpkA4SH396+gfsQRgzAF9+umn9O/fH7/fT3p6Oi1atEioJADlJBFUrFiRLVu2UKNGjbhLBqrKli1bDm5dEn8muuANwvOnQDiAR52rf5EIR/8YY9iyZQt33XUXr732Gs2bN2fGjBlxs0hcSSsXiaBu3bqsXbuWTZs2xTqUQ1KxYkXq1q1b/IH+TPjxDcLz/pwAVEAPNNvXGPMnuYvErVixggcffJCHHnoorhaJK2nlIhEkJSWV/6qcP5PwpO4QzEYKJABr/jEmMps2baJGjRp4vV6efPJJ6tevn7f8SyKL/7nRCeK3byZAYJ8z/j8vASQj6TfA9R/Dhc9YEjCmCKrKq6++StOmTXnppZcA6NGjhyUBV7moEZRb/kz2/PI1/10T5Jysd/NGAYknCdr2thqAMRHIysqif//+fP7552RkZNC5c+dYh1TmWCIoi9zRQDp/CsnhIOfizAZ2usEF2l7r1ACMMQc0efJkBg4ciIgwduxYbr755nKxSFxJs0RQ1vgzCU/sjoSy93cG558NnLvDlzGmWLVr16ZTp068+OKL1KtXL9bhlFmWCMoSfybrpz7CMcFsZy6AgM0GNiZygUCAp556ilAoxODBg+natStdu3aNdVhlniWCMiI4ewLy8T0coyE8AioexOOz0UDGRGjevHnceOON/Pjjj1xzzTVxPcm0tFkiiDV/Jtlzp+Bd8BoeDbtNQYI0PBPOvN8SgDHF2Lt3L48++ijDhw+nVq1afPjhh3G9bWQsRLXXRETOE5FlIrJCRO4r5PV6IvKViMwXkYUicn404ylz3LkBSQsm4XWTAOD0B1gSMCYiK1euZOTIkVx//fUsWbLEksAhiFoiEBEvMAboBqQBV4tIWoHDHgLeUdWTgJ7A2GjFUxbtnPUaBPfPDQDA44PzR1gSMOYAdu7cycSJEwFo2bIly5cv5+WXX06IlUKjIZo1gnbAClVdqao5wFtAjwLHKHCUe78q8FsU4yk7/JlseWsQFRe/gZBvbkD6jXDDfyD9+hgHaEzZNX36dFq1akXfvn1ZunQpQLnaNjIWotlHcDzgz/d4LdC+wDFDgM9E5DbgCODswj5IRPoD/YH4HwLmzyQ0sTtHB7Pz7RVgcwOMKc7mzZu58847ef3110lLS2PmzJkJu0hcSYv1zIqrgYmqWhc4H5gsIn+JSVXHq2q6qqbXqlWr1IMsMf5Mtv9nKLjDQx0Cvoo2N8CYA8hdJO6tt95i8ODBzJs3j1NPPTXWYZUb0awRrANS8j2u6z6XX1/gPABV/V5EKgI1gY1RjCs25kwk/PHdVAkHbXioMRH6/fffqVWrFl6vl+HDh1O/fn1at24d67DKnWjWCGYDTUSkgYgk43QGTytwzBqgC4CItAAqAvG5lvQBhFf/QPDfdyHhIN7cSWINz7TF4owpgqryyiuv0KxZM8aPHw9A9+7dLQlESdQSgaoGgVuBT4GlOKODFovIUBG5yD3sbuAmEfkReBO4Xg95u64yyp+J/8PBiIbyLRpnw0ONKcrKlSs5++yz6devH23atOHsswvtOjQlKKoTylR1OjC9wHOD891fAnSMZgwx5TYH1c1tDkKcJGDDQ40p1KRJkxg0aBBer5cXX3yRm266yRaJKwU2szha/JmEp9+N5E8CjTpbTcCYA6hTpw5nnXUWL7zwQmS79pkSYYkgGvyZ7P38MZLzkoA1BxlTmJycHJ544gnC4TBDhgzhnHPO4Zxzzol1WAnHEkFJy102IpiNB2sOMqYos2fP5sYbb2TRokX07t3bFomLIWt8K2E5c6dAcB8+FPA4zUE2W9iYPHv27OGee+7h1FNPZdu2bUybNo3XXnvNkkAMWSIoQbrmB2TBFETd5iBvkjUHGVPAqlWrGD16NDfddBOLFy+me/fusQ4p4VkiKEGrv3oFrwYQAUHgpGssCRgD7Nixg1dffRVwFolbsWIFL774IlWrVo1xZAYsEZQMfya73r+NOqvey5srYFtKGuP4+OOPadmyJf369ePnn38GICUlpZh3mdJkieBw+TPRSd2p/NNkkgghWG3AGIBNmzbRq1cvLrzwQqpVq8b3339P8+bNYx2WKYSNGjpMoQVvIMFsPKhTE7BF5IwhFApx+umns2rVKh599FHuu+8+kpOTYx2WKYIlgsOgc16FuZMQdTaaF08StO1ti8iZhLVhwwaOOeYYvF4vI0aMIDU1lVatWsU6LFMMaxo6VP5Mwh/fnbfPsOTfU8CSgEkw4XCYcePG0bRpU8aNGwfAhRdeaEkgTkSUCESkkog0i3Yw8eT3hV+g4fD+zmGP15qDTEJasWIFXbp0YcCAAZxyyimce+65sQ7JHKRiE4GIdAcWAJ+4j9uISMHlpBPO1/4gisedOWz7DJvE9Oqrr3LCCScwb948XnrpJb744gsaNmwY67DMQYqkRjAEZ//h7QCqugBoEMWYyrw9373CpeufxSuh/ctH2Mxhk4Dq1avHueeey5IlS+jXr5/NDo5TkXQWB1R1R4E/cPnaM+Bg+DOp+Pm9iDtUFA3D3i2xjsqYUpGdnc3jjz9OOBxm6NChdOnShS5dusQ6LHOYIqkRLBaRawCviDQRkdHAd1GOq+zKmoHm22QG8UBqRqyjMibqfvjhB04++WQeffRR1qxZQ3nbQyqRRZIIbgNaAtnAG8AO4PZoBlWWrcupRFg9hK1vwCSI3bt3c9ddd9GhQwd27NjBv//9byZOnGjNQOVIJIngAlV9UFVPcW8PARcV+65ySNf8QM3/DUEkbH0DJmGsXr2asWPHMmDAABYvXswFF1wQ65BMCYskEdwf4XPl3uqvXiEpnI0PRVStb8CUW9u3b+fll18GIC0tjRUrVjB27FiOOuqoGEdmoqHIzmIR6QacDxwvIqPyvXQUEIx2YGVNIGsWx616P98G9D7rGzDl0tSpUxk4cCAbN27k9NNPp3nz5rZtZDl3oBrBb8AcYB8wN99tGpBwM0Z++2YCSRq0ReVMubVx40Z69uzJxRdfTK1atZg1a5YtEpcgiqwRqOqPwI8i8oaqBkoxprLHn8lxqz7YXxuwJaZNORMKhejYsSNr1qzhscce49577yUpKSnWYZlSEsk8glQReRxIAyrmPqmqCTN9cNPMidRwN5zBagOmHPntt9849thj8Xq9PPfcc6SmppKWlhbrsEwpi6Sz+FXgBZx+gc7Aa8Dr0QyqLNE1P1D157dtwxlTroTDYV544QWaN2/Oiy++CMD5559vSSBBRZIIKqnql4Co6mpVHQIkzPixDQu/cFYYxfoGTPnwyy+/0LlzZwYNGkT79u3p1q1brEMyMRZJIsgWEQ+wXERuFZFLgCOjHFeZsXibD0VQ8diGMybuvfLKK5x44oksXLiQCRMm8Nlnn9GgQUIvHWaILBHcDlQG/gacDFwL9IlmUGWGP5OMX4fjkTAiHjjvCasNmLiWmppKt27dWLJkCTfccIPNDjZAMZ3FIuIFrlLVe4BdwA2lElVZ4M9k078fpboG8IqCTSAzcSg7O5t//vOfADz22GO2SJwp1AFrBKoaAk4vpVjKDndD+mq/z8Qj6jQLeZNtApmJK9999x1t2rTh//7v/1i/fr0tEmeKFMnw0fnuRjTvArtzn1TVD6IWVaxlzUCDOfhQFA/S8Ew4835rFjJxYdeuXTz44IOMHj2alJQUPvnkE9s1zBxQJH0EFYEtwFlAd/d2YSQfLiLnicgyEVkhIvcVccyVIrJERBaLyBuRBh5VlWoQdnccEF8FSwImrqxZs4Zx48Zxyy23sGjRIksCpljF1ghU9ZD6Bdz+hTHAOcBaYLaITFPVJfmOaYKzgF1HVd0mIsccyneVKH8m4U/+4Ww447EOYhMftm3bxrvvvkv//v1JS0tj5cqV1KlTJ9ZhmTgR0eb1h6gdsEJVV6pqDvAW0KPAMTcBY1R1G4CqboxiPJHJmgHBbHzi7EhsHcSmrPvwww9JS0tj0KBBLFu2DMCSgDko0UwExwP+fI/Xus/l1xRoKiIzRWSWiJxX2AeJSH8RmSMiczZt2hSlcB1aqTqCOmsKaRgq1Yjq9xlzqDZs2MAVV1zBpZdeyrHHHktmZibNmjWLdVgmDkXSWRzt728CnAnUBb4VkRNUdXv+g1R1PDAeID09PapDH3Zs+Z0jVfCJAh6rEZgyKRQKkZGRgd/vZ9iwYdxzzz22SJw5ZMUmAhGpDQwD6qhqNxFJAzqo6ivFvHUdkJLvcV33ufzWAj+4q5uuEpFfcBLD7EhPoKSt3luBNDwo6nQU25BRU4asXbuWOnXq4PV6GTVqFA0aNLClos1hi6RpaCLwKZDb6PgLcEcE75sNNBGRBiKSDPTE2csgv3/h1AYQkZo4TUUrI/js6PBnkrbwcQTrKDZlSzgcZvTo0TRv3pwXXngBgG7dulkSMCUikkRQU1XfAcIAqhoEQsW9yT3uVpwkshR4R1UXi8hQEcnd8/hTYIuILAG+Av6uqrFri8magSccwCe2FaUpO37++Wc6derE3/72N04//XQuvCGYKIsAACAASURBVDCi0dvGRCySPoLdIlIDdxVmETkV2BHJh6vqdGB6gecG57uvwF3uLfZSMwjiRVA8thWlKQNefvllbr31VipXrsykSZPo3bu3rQ9kSlwkieBunCadRiIyE6gFXB7VqGJk655sjlAFATfvGRNTjRo1onv37jz//PPUrl071uGYciqSCWVzReQMoBlOEbmsvG5duXPWZI4m6LSXhUPOnALrIzClaN++fQwdOhSAYcOG0blzZzp37hzjqEx5V2wfgYgsBO4F9qnqovKaBPBncvyqDxDcuoA1DZlSNnPmTNq0acPjjz/Opk2bbJE4U2oi6SzujrNN5TsiMltE7hGRelGOq/RlzcBDCBHbicyUrj/++IPbbruNjIwMsrOz+fTTT3nppZesL8CUmmITgbs95VOqejJwDdAaWBX1yErZdqoQViGM7URmStfatWt5+eWXue222/jpp5/o2rVrrEMyCSaimcUiUh+4yr2FcJqKyg9/Jkd+9TAQBvHa/AETdVu2bOGdd95h4MCBtGjRgpUrV3LcccfFOiyToCKZWfwDkISzH8EVqhq7CV/RkjUDCefgFXVWGbL5AyZKVJX333+fW265ha1bt3LWWWfRrFkzSwImpiLpI7hOVduq6uPlMgkApGYQwksYQayT2ETJ+vXrueyyy7jiiitISUlhzpw5tkicKROKrBGIyLWq+jpwgYhcUPB1VR0Z1chKUSiszggNmz9goiR3kbh169bx1FNPceedd+LzxXrNR2McB/qXeIT7s0ohr5Wr0nLn0q+oQtjZf8DmD5gS5Pf7Of744/F6vYwZM4YGDRrQtGnTWIdlzJ8U2TSkquPcu1+o6qP5b8CXpRNe6VhzVFsC+FDx2ib1pkSEQiFGjRr1p0Xizj33XEsCpkyKpI9gdITPxa1Nu/bxfiiDXS17QZ9pVhswh2Xp0qVkZGRw++23c8YZZ9C9e/dYh2TMAR2oj6ADcBpQS0TyLwp3FOCNdmClxp/JGbP6Id4A3p8rQPvesY7IxLHx48dz2223UaVKFSZPnkyvXr1sYpgp8w5UI0gGjsRJFlXy3XZSnhady1t6OoyEcpz+AWMOUZMmTbjkkktYsmQJ1157rSUBExeKrBGo6jfANyIyUVVXl2JMpSpcsbozbBQPHusfMAdp7969DBkyBBHhiSeesEXiTFw6UNPQs6p6B/C8iPxllJCqXlTI2+KLPxM+uQ/RMOqxGcXm4Hz77bf069eP5cuXM2DAAFTVagAmLh1o+Ohk9+fw0ggkJrJmQCgHn80oNgdh586d3Hfffbzwwgs0bNiQL7/8krPOOivWYRlzyA7UNDTX/flN7nMiUg1IUdWFpRBb9KVmEBYvGlY8XptRbCLz22+/MXHiRO666y6GDh3KEUccUfybjCnDIllr6GvgIvfYucBGEZmpqmVje8nDFFbwCUj5miNnStjmzZt55513GDRoEM2bN2fVqlW2Y5gpNyKZR1BVVXcClwKvqWp74OzohlVK3D0I/jSj2Jh8VJW3336btLQ07rjjDn755RcASwKmXIkkEfhE5DjgSuDfUY6nVGXXPY2A+ghjM4rNX/32229cfPHF9OzZk/r16zN37lybGWzKpUhWvRoKfArMVNXZItIQWB7dsErH+h37+F8og46Na9KgSz8bMWTyhEIhOnXqxLp16xg+fDi33367LRJnyq1INq9/F2cvgtzHK4HLohlUqfBnkvJRT3p6c/D4k4F+sY7IlAGrV6+mbt26eL1exo4dS8OGDWncuHGswzImqiLZvL6uiHwoIhvd2/siUrc0gouqrBlIKMeZURwOWP9AgguFQowcOZIWLVrkLRLXtWtXSwImIUTSR/AqMA2o494+cp+Lb7YZjXEtWrSI0047jbvvvpsuXbpw8cUXxzokY0pVJImglqq+qqpB9zYRqBXluEpFOG/IqA0dTVQvvvgibdu2ZeXKlbzxxhtMmzaNunXjv8JrzMGIJBFsEZFrRcTr3q4F4n4K7r7l3+DVsA0dTVCqTvJv0aIFV1xxBUuWLOHqq6+2JSJMQopkGMSNOPsPPOM+ngncELWISsmmGqdQE58zj8CGjiaMPXv2MHjwYLxeL08++SRnnHEGZ5xxRqzDMiamiq0RqOpqVb1IVWu5t4tVdU1pBBdNW3fn8H4og41Nr7LNaBLE119/TevWrRkxYgS7du3KqxUYk+giGTXUUEQ+EpFN7qihqe5cgvjlz6TlF73p6f2KY1Z+EOtoTJTt2LGDm2++OW956P/+97+MGTPGmoGMcUXSR/AG8A5wHM6ooXeBN6MZVNRlzcATdoeOhmzoaHm3fv16Xn/9de655x4WLlxo+wUYU0AkiaCyqk7ON2rodaBiJB8uIueJyDIRWSEi9x3guMtEREUkPdLAD4sNHS33Nm3axOjRztbazZs3Jysri6effprKlSvHODJjyp5IEsF/ROQ+EUkVkfoici8wXUSqi0j1ot4kIl5gDNANSAOuFpG0Qo6rAtwO/HBop3BobOho+aSqvPHGG7Ro0YK77747b5G4WrXKxYhnY6IikkRwJXAz8BXwNTAQ6ImzJPWcA7yvHbBCVVeqag7wFtCjkOP+CTwJ7Is87MOz55evbehoOeT3++nevTu9evWicePGzJ8/3xaJMyYCkaw11OAQP/t4wJ/v8Vqgff4DRKQtzkY3H4vI34v6IBHpD/QHqFev3iGGs9+ao9pSHx8esaGj5UUwGOTMM89kw4YNPPPMM9x22214vd5Yh2VMXIjZcooi4gFGAtcXd6yqjgfGA6Snpx92W85iT3MeyHmAl8/YR/WWXWzoaBzLysoiJSUFn8/HuHHjaNiwIQ0bxvegNmNKWyRNQ4dqHZCS73Fd97lcVYBWwNcikgWcCkwrjQ7jnKzv6eBdSpXmnS0JxKlgMMjw4cNp0aIFY8eOBeDss8+2JGDMIYhmjWA20EREGuAkgJ7ANbkvquoOoGbuY3dLzHtU9UD9DofPn8lli27B4wvge32aTSaLQwsXLqRv377MmTOHHj16cNll8b8qujGxFMmEMnHXGhrsPq4nIsWWnKoaBG7F2dRmKfCOqi4WkaEictHhBn7Ismbg1QA+whDKsY7iODN27FhOPvlkVq9ezdtvv82HH35InTp1Yh2WMXEtkhrBWCAMnIWzW9kfwPvAKcW9UVWnA9MLPDe4iGPPjCCWw5eaQRAfEMRrHcVxQ1UREVq1akXPnj155plnqFmzZvFvNMYUK5JE0F5V24rIfABV3SYiyVGOK3pS2vGs70Z6VJhL887XWrNQGbd7924eeughfD4fTz/9NJ06daJTp06xDsuYciWSzuKAOzlMAUSkFk4NIT75M/lbYAJNd8+FT+4Df2asIzJF+PLLLznhhBN49tlnyc7OtkXijImSSBLBKOBD4BgR+T/gf8CwqEYVRdkrviGJIB7rIyiztm/fTr9+/Tj77LPx+Xx8++23jBo1yhaJMyZKIplQNkVE5gJdAAEuVtWlUY8sSjbVOIUatg9Bmfb777/z1ltv8Y9//INHHnmESpUqxTokY8q1YhOBiNQD9uDsVZz3XLzuSbB1dw5fhzI4O602x2bcYH0EZURu4X/77bfTrFkzsrKyrDPYmFISSWfxxzj9A4Kz6mgDYBnQMopxRYc/k7TPe5PmzcGzMhky4n6jtbinqkyZMoXbb7+dXbt2cf7559OkSRNLAsaUokh2KDtBVVu7P5vgLCb3ffRDiwLbh6BMWbNmDRdccAG9e/emWbNmLFiwgCZNmsQ6LGMSzkHPLFbVeSLSvvgjy6DUDIKShEcD+Kx/IKZyF4nbuHEjo0aNYtCgQbZInDExEkkfwV35HnqAtsBvUYsomlLa8Xq1gZz4xzecfO711j8QAytXrqR+/fr4fD5eeuklGjVqRGpqaqzDMiahRTJ8tEq+WwWcPoPC9hUo+/yZXLN1LG0CP9ocglIWDAZ58sknSUtLY8yYMQB06dLFkoAxZcABawTuRLIqqnpPKcUTXVkz8BHEm38OgdUKom7BggX07duXefPmcckll3DFFVfEOiRjTD5F1ghExKeqIaBjKcYTXakZBNRHCC9YH0GpeP755znllFNYt24d7733Hh988AHHHXdcrMMyxuRzoBpBJk5/wAIRmQa8C+zOfVFVP4hybCUuUCedRwO9GXjMYuqffrXVBqIod5G41q1b06tXL0aOHEn16kVucW2MiaFIRg1VBLbgrD6aO59AgbhLBPtWfs8jSZOpsD0In/wItdMsGZSwXbt28eCDD5KUlMTw4cNtkThj4sCBOouPcUcMLQJ+cn8udn8uKoXYSlx41QxbZyiKPvvsM1q1asXo0aMJBAK2SJwxceJANQIvcCRODaCguPwfvqdOB5JtnaESt23bNu666y4mTpxIs2bN+Pbbbzn99NNjHZYxJkIHSgTrVXVoqUVSCvbUPplbch7gsZO2k9bhAmsWKiEbN27kvffe4/7772fw4MFUrFgx1iEZYw7CgRJBuVvzNxR2KjK2nPHh27BhA2+++SZ33nln3iJxNWrUiHVYxphDcKA+gi6lFkUpSf5tDlOSh9Fs8XMw6SKbUHYIVJVJkyaRlpbG/fffz/LlywEsCRgTx4pMBKq6tTQDKQ3J676zzuLDkJWVxXnnncf1119PWlqaLRJnTDlx0IvOxbPNNU+hGj68EkKss/igBINBOnfuzObNmxkzZgwDBgzA44lkhRJjTFmXWIng6BMZkvMAz3fcQ50Tz7HO4gisWLGCBg0a4PP5mDBhAg0bNqR+/fqxDssYU4IS6pKuwvq5nOpZSjCloyWBYgQCAYYNG0bLli3zFonr3LmzJQFjyqHEqRH4M2k/4wba+3LwfDQVqn9kyaAI8+bNo2/fvixYsIArrriCq666KtYhGWOiKHFqBLY7WURGjRpFu3bt2LBhAx988AHvvPMOtWvXjnVYxpgoSpxEkJpB2JNEUD3gTbKO4gJyl4M46aSTuO6661iyZAmXXHJJjKMyxpQGibf1YNLT03XOnDmH9N6pH33Isln/4dYbb6Byow4lHFl8+uOPP7j//vupUKECI0aMiHU4xpgoEZG5qppe2GuJUyMAAiEn6flsa1wAPvnkE1q1asXYsWNRVVskzpgElTiJwJ9Jj4UDuMv3LkmvX5LQs4q3bNlCnz596NatG0cccQQzZ85k5MiRtvSGMQkqcRJB1gy84YDbWZzYs4q3bNnChx9+yMMPP8z8+fPp0MGayYxJZFFNBCJynogsE5EVInJfIa/fJSJLRGShiHwpItEbpJ6aQUhyO4sTb1bx+vXrGT58OKpK06ZNWb16NUOHDqVChQqxDs0YE2NRSwTuxvdjgG5AGnC1iKQVOGw+kK6qrYH3gKeiFQ8p7ZjQ+Dle8PSEPtMSZg6BqjJhwgRatGjBww8/zIoVKwCoVq1ajCMzxpQV0awRtANWqOpKVc0B3gJ65D9AVb9S1T3uw1lA3SjGw4rkNN5IvjxhksCqVavo2rUrffv25cQTT+THH3+0ReKMMX8RzZnFxwP+fI/XAu0PcHxf4D+FvSAi/YH+APXq1TvkgLKDYSr4EqNbJBgMctZZZ7FlyxZeeOEF+vfvb4vEGWMKVSaWmBCRa4F04IzCXlfV8cB4cOYRHOr3HL/rJ9oEZ4P/iHJbK1i+fDkNGzbE5/Px6quv0qhRI1JSUmIdljGmDIvmJeI6IH8JVNd97k9E5GzgQeAiVc2OWjT+TO747R76ZL9eLjelCQQCPPbYY7Rq1Yrnn38egDPPPNOSgDGmWNFMBLOBJiLSQESSgZ7AtPwHiMhJwDicJLAxirFA1gx8GsBbDjelmTNnDunp6Tz88MNceumlXH311bEOyRgTR6KWCFQ1CNwKfAosBd5R1cUiMlRELnIPexo4EnhXRBaIyLQiPu7wpWYQJIkQ5Wv46HPPPUf79u3ZvHkzU6dO5c033+SYY46JdVjGmDgS1T4CVZ0OTC/w3OB898+O5vf/SUo7Bh89jJN1MVdefnXc9xGoKiJCeno6ffv25amnnuLoo4+OdVjGmDhUJjqLS8siTzO2VD2RK1NOiXUoh2znzp384x//oGLFijzzzDN07NiRjh07xjosY0wcS6jxhI2zl3DhjrfitqN4+vTptGzZkvHjx+Pz+WyROGNMiUicRODP5MndD9F964S4GzW0efNmrr32Wi644AKqVq3Kd999x9NPP22LxBljSkTiJIKsGfgIxuWooW3btvHRRx/xyCOPMG/ePNq3P9C8PGOMOTiJ00eQmkEQH0IQbxyMGlq3bh1Tpkzh73//O02aNGH16tXWGWyMiYrEqRGktKOvDubrOv3L9KJzqspLL71EWloaQ4YM4ddffwWwJGCMiZrESQRAZrARc+rdUGaTwK+//kqXLl3o378/bdu2ZeHChTRu3DjWYRljyrmEaRoKh5UTwss4ff0M8F9S5pJBMBikS5cubN26lXHjxtGvXz9bJM4YUyoSJhEEVs9iSvIwKqwOwqRXy0zz0LJly2jUqBE+n49JkybRqFEj6taN6mrcxhjzJwlzyRleNYMkgnjKyKihnJwcHn30UU444QTGjBkDwBlnnGFJwBhT6hKmRrDnuA5UxoeHEJ4YjxrKzMykb9++LFq0iGuuuYZevXrFLBZjjEmYGsHOWm3plfMAS1vcFtNmoWeffZYOHTrkzQ2YMmUKNWvWjEksxhgDCZQI9gVCzNOmrEkbEJMkkLscRLt27bjppptYvHgxF154YanHYYwxBSVM09C+QAiAikneUv3eHTt2cO+991KpUiWeffZZTjvtNE477bRSjcEYYw4kgWoEYQAqJJXeKX/00UekpaXx8ssvU6FCBVskzhhTJiVMIkheP4dB3qnU2LYg6t+1adMmrrnmGi666CJq1KjBrFmzePLJJ22ROGNMmZQYTUP+TNp8dR0n+nLw/Gcq1P4oqv0EO3bsYPr06Tz66KPcd999JCcnR+27jDHmcCVGjSBrBhLKwSdhJByIyhwCv9/P448/jqrSuHFjVq9ezeDBgy0JGGPKvMRIBKkZhD3JBNWDepNKdA5BOBzmxRdfpGXLljz22GN5i8RVrVq1xL7DGGOiKTESQUo7ZnZ8hZHBK9jQ450SaxZavnw5Z511FgMHDqRdu3b89NNPtkicMSbuJEYfAbD56DaMDQk965TMfsXBYJBzzjmH7du388orr3DDDTdYZ7AxJi4lTCIIuUM3D7esXrp0KU2aNMHn8zF58mQaNWpEnTp1SiBCY4yJjcRoGsJZhhrA6zm0TJCdnc0jjzxC69atef755wHIyMiwJGCMiXsJUyNw88AhJYJZs2bRt29flixZQu/evendu3cJR2eMMbGTMDWCalsXMMg7laT1sw/qfSNGjOC0007jjz/+YPr06bz22mvUqFEjSlEaY0zpS4xE4M/knNn9uMv3LtXevRz8mcW+JRx2lqTo0KEDAwYMYNGiRXTr1i3akRpjTKlLjKahrBl4wgE8EkZD7oSyIoaQbt++nbvvvpvKlSszevRoWyTOGFPuJUaNIDWDsCeJoHrgABPK/vWvf5GWlsakSZOoUqWKLRJnjEkIiZEIUtrxn7bjGRm8gt1XffiX2sDGjRu58sorueSSS6hduzaZmZkMGzbM5gUYYxJCYiQC4PeqrRkb6kGo7l8nlO3cuZPPP/+c//u//yMzM5O2bdvGIEJjjImNxOgjyCf3In/NmjVMnjyZBx54gMaNG7NmzRqqVKkS2+CMMSYGolojEJHzRGSZiKwQkfsKeb2CiLztvv6DiKRGK5bc5n4Nhxk7diwtW7Zk2LBheYvEWRIwxiSqqCUCEfECY4BuQBpwtYikFTisL7BNVRsDzwBPRiueWjuceQT39uzELbfcQocOHVi8eLEtEmeMSXjRbBpqB6xQ1ZUAIvIW0ANYku+YHsAQ9/57wPMiIlrSw3X8mVwwfwAX+gLknBDmovMHc/7NQ6wz2BhjiG7T0PGAP9/jte5zhR6jqkFgB/CXabsi0l9E5ojInE2bNh18JFkz8IYD+CRMpWQvF7Q8ypKAMca44mLUkKqOV9V0VU2vVavWwX9AagYeXwUQL+JNLtGNaYwxJt5Fs2loHZCS73Fd97nCjlkrIj6gKrClxCNJaQd9pjkzilMzorpfsTHGxJtoJoLZQBMRaYBT4PcErilwzDSgD/A9cDnw3xLvH8iV0s4SgDHGFCJqiUBVgyJyK/Ap4AUmqOpiERkKzFHVacArwGQRWQFsxUkWxhhjSlFUJ5Sp6nRgeoHnBue7vw+4IpoxGGOMObC46Cw2xhgTPZYIjDEmwVkiMMaYBGeJwBhjEpzE2+YrIrIJWH2Ib68JbC7BcOKBnXNisHNODIdzzvVVtdAZuXGXCA6HiMxR1fRYx1Ga7JwTg51zYojWOVvTkDHGJDhLBMYYk+ASLRGMj3UAMWDnnBjsnBNDVM45ofoIjDHG/FWi1QiMMcYUYInAGGMSXLlMBCJynogsE5EVInJfIa9XEJG33dd/EJHU0o+yZEVwzneJyBIRWSgiX4pI/VjEWZKKO+d8x10mIioicT/UMJJzFpEr3b/1YhF5o7RjLGkR/NuuJyJfich899/3+bGIs6SIyAQR2Sgii4p4XURklPv7WCgibQ/7S1W1XN1wlrz+FWgIJAM/AmkFjhkEvOje7wm8Heu4S+GcOwOV3fsDE+Gc3eOqAN8Cs4D0WMddCn/nJsB8oJr7+JhYx10K5zweGOjeTwOyYh33YZ5zJ6AtsKiI188H/gMIcCrww+F+Z3msEbQDVqjqSlXNAd4CehQ4pgcwyb3/HtBF4nsT42LPWVW/UtU97sNZODvGxbNI/s4A/wSeBPaVZnBREsk53wSMUdVtAKq6sZRjLGmRnLMCR7n3qwK/lWJ8JU5Vv8XZn6UoPYDX1DELOFpEjjuc7yyPieB4wJ/v8Vr3uUKPUdUgsAOoUSrRRUck55xfX5wrinhW7Dm7VeYUVf24NAOLokj+zk2BpiIyU0Rmich5pRZddERyzkOAa0VkLc7+J7eVTmgxc7D/34sV1Y1pTNkjItcC6cAZsY4lmkTEA4wEro9xKKXNh9M8dCZOre9bETlBVbfHNKrouhqYqKojRKQDzq6HrVQ1HOvA4kV5rBGsA1LyPa7rPlfoMSLiw6lObimV6KIjknNGRM4GHgQuUtXsUootWoo75ypAK+BrEcnCaUudFucdxpH8ndcC01Q1oKqrgF9wEkO8iuSc+wLvAKjq90BFnMXZyquI/r8fjPKYCGYDTUSkgYgk43QGTytwzDSgj3v/cuC/6vbCxKliz1lETgLG4SSBeG83hmLOWVV3qGpNVU1V1VScfpGLVHVObMItEZH82/4XTm0AEamJ01S0sjSDLGGRnPMaoAuAiLTASQSbSjXK0jUNuM4dPXQqsENV1x/OB5a7piFVDYrIrcCnOCMOJqjqYhEZCsxR1WnAKzjVxxU4nTI9Yxfx4YvwnJ8GjgTedfvF16jqRTEL+jBFeM7lSoTn/CnQVUSWACHg76oat7XdCM/5buAlEbkTp+P4+ni+sBORN3GSeU233+MRIAlAVV/E6Qc5H1gB7AFuOOzvjOPflzHGmBJQHpuGjDHGHARLBMYYk+AsERhjTIKzRGCMMQnOEoExxiQ4SwSmzBKRkIgsyHdLPcCxu0ovsqKJSB0Rec+93yb/SpgictGBVkmNQiypInJNaX2fiV82fNSUWSKyS1WPLOljS4uIXI+z4umtUfwOn7teVmGvnQnco6oXRuv7TflgNQITN0TkSHcvhXki8pOI/GW1URE5TkS+dWsQi0Qkw32+q4h87773XRH5S9IQka9F5Ll8723nPl9dRP7lrv0+S0Rau8+fka+2Ml9EqrhX4YvcWbBDgavc168SketF5HkRqSoiq931kBCRI0TELyJJItJIRD4RkbkiMkNEmhcS5xARmSwiM3EmRqa6x85zb6e5hz4BZLjff6eIeEXkaRGZ7Z7LzSX0pzHxLtZrb9vNbkXdcGbGLnBvH+LMhD/Kfa0mzszK3FrtLvfn3cCD7n0vzppDNXH2JDjCff4fwOBCvu9r4CX3fifc9eCB0cAj7v2zgAXu/Y+Aju79I934UvO973rg+Xyfn/cYmAp0du9fBbzs3v8SaOLeb4+z/EnBOIcAc4FK7uPKQEX3fhOcGbfgzE79d7739Qcecu9XAOYADWL9d7Zb7G/lbokJU67sVdU2uQ9EJAkYJiKdgDDO0ru1gQ353jMbmOAe+y9VXSAiZ+BsWDLTXV4jGfi+iO98E5w14UXkKBE5GjgduMx9/r8iUkNEjgJmAiNFZArwgaqulci3tXgbJwF8hbPEyVi3lnIa+5cBAafALsw0Vd3r3k8CnheRNjjJs2kR7+kKtBaRy93HVXESx6pIgzblkyUCE096AbWAk1U1IM6qohXzH+AW4J2AC4CJIjIS2AZ8rqpXR/AdBTvNiuxEU9UnRORjnHVfZorIuUS+Ac40nKRWHTgZ+C9wBLA9f/I7gN357t8J/A6ciNPcW1QMAtymqp9GGKNJENZHYOJJVWCjmwQ6A3/Zd1mcvZh/V9WXgJdxtvybBXQUkcbuMUeISFFXzVe5x5yOs6rjDmAGThLK7YDdrKo7RaSRqv6kqk/i1EQKtuf/gdM09Requst9z3M4zTchVd0JrBKRK9zvEhE5McLfy3p11t/vjdMkVtj3fwoMdGtLiEhTETkigs835ZzVCEw8mQJ8JCI/4bRv/1zIMWcCfxeRALALuE5VN7kjeN4Ukdymlodw1uovaJ+IzMdpbrnRfW4ITnPTQpzVHnOXML/DTUhhYDHOrm/5twz8CrhPRBYAjxfyXW8D77ox5+oFvCAiD7kxvIWzT++BjAXeF5HrgE/YX1tYCIRE5EdgIk7SSQXmidP2tAm4uJjPNgnAho8a4xKRr3GGW8bzngXGHDRrGjLGmARnNQJjjElwViMwwmUFpwAAACJJREFUxpgEZ4nAGGMSnCUCY4xJcJYIjDEmwVkiMMaYBPf/0uywwGioOQgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI_fYCnQTvD8",
        "outputId": "e117600c-459c-4641-d002-ecde0e6688b7"
      },
      "source": [
        "auc(pd.DataFrame(y_testlabel).values.flatten()==\"s\", y_pred_grd_rfcat)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4074.1749999999997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3U3aTZiT84o"
      },
      "source": [
        "Questo numero andrebbe comparato con quello del gbt per vedere le performance."
      ]
    }
  ]
}