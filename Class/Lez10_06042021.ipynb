{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lez10 06042021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMmXajgEAoIU4Np79j0+9Ih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedeleDelvecchio/MLPNS/blob/main/Class/Lez10_06042021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdO_ve7G7sze"
      },
      "source": [
        "**Oggi parliamo di come interpretare il risultato ottenuto l'altra volta sul gamma ray burst, utilizzando una broken power low.**\n",
        "\n",
        "Poi parleremo di clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkvdmJb09wSC"
      },
      "source": [
        "Finiamo la discussione su MCMC:\n",
        "\n",
        "La broken power law è la curva migliore per rappresentare i gamma ray burst. Dobbiamo vedere come è venuto il fitting, e come abbiamo fatto il model selection dopo.\n",
        "\n",
        "For this model we work in physical time space, unlike the line model which use the log time. \n",
        "\n",
        "The astrophysic use the magnitude space wich is the log of power low.\n",
        "Tb is the break time, and it define a transition between the time in which dominate a1 and the time in which dominate a2. \n",
        "\n",
        "\n",
        "So we work in magnitude vs time space (see graphic), using m = b -log10((t/Tb)^-a1 + (t/Tb)^-a2). We use this form to have a smooth transition between the two regime. If we use F = F0/(t/Tb)^-a1 + (t/Tb)^-a2 we will have a non smooth curve.\n",
        "\n",
        "The reason we use broken power low is that the sharp transition is unphysical.\n",
        "\n",
        "\n",
        "####We used emcee package\n",
        "\n",
        "To use it we create the log_probability that is the posterior that we were smapling. So this is our targer.\n",
        "###**We were sampling the posterior porbability of the parameter given the data.**\n",
        "\n",
        "**The posterior is proportional to: the likelihood times the prior** So this in log space this became a sum, so we return (lp + log_likelihood) log_probability, that is the posterior we were sampling. \n",
        "\n",
        "We set a prior that si loosely infomrative: means that we remove the region of parameter space that is unphysical. Uniformative prior is a probab distribution for the model that is a flat distribution within some regime. \n",
        "\n",
        "**note that the prior should be based on knowledge indipendent of the data set we have**. \n",
        "This means we have to do our fit not looking at the shape of the data, bue only based on the physical assumption.\n",
        "\n",
        "The log_llikelihood is the chi square distribution, so the likelihood is the exponent of chi square distribution. This come from that I believe that the generative process is gaussian. \n",
        "\n",
        "I initialize a bounch of walker wich explore the space at the same time. The num of walker must be high then my dimension, say nwalker=ndim^2, or ndim^3. \n",
        "My chain will be long enough to stabilize. Once I pass the stabilization process I can pick a shorter chain to improve computational afficience. \n",
        "And I also do an initial guess to be in a reasonable place, that we had bacause of the previous fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_D3Bjzt-Kox"
      },
      "source": [
        "I discard the initial point to take only the point that has passed the initial limited performance, which has passed the stabilization point. If I don't throw away the initial point, my probability distribution may ahve a larger range, and the visualization will suffer.\n",
        "\n",
        "The chain are axploring the surface, and ther are landing on position with higer probabiltiy, if those positoin have a  higer porbabilty themselves.  In a sens, I start with the bayes theorem with the posterior proportional to likelihood and prior and I land in a frequentist sense on the posterioir: the more time a walker land ina  place in the postirior, the more porbable that place is.\n",
        "\n",
        "**We must choose the median intead of the mean for 2 reason:**\n",
        "- avoid problem with the descarding initial point, which are far from the probability distribution.\n",
        "- starting from the wrong place. Cause median is less sensitive to the outliers.\n",
        "\n",
        "The probability distribution of each variable are **MARGINAL DISTRIBUTION** showed in the graphic.\n",
        "There is a strong covariance between the slope and the intercept, as we see from the graphic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGkHOGvDDJVs"
      },
      "source": [
        "####**HOW WE EVALUATE IF OUR MCMC WORKED WELL?**\n",
        "\n",
        "the mcmc compare with gradient descent, insted of just optimization that might get me stuced inocal minimum because I never go in direction in which posterior probab decreases, instead of going on the lower posterior, I see if it is convenient go there before doing the step to go there. \n",
        "\n",
        "**I.e. I explore the surface even if the direction is in the decreasing od the posterior, but I do this with lower porbabilty that other better direction. Infact I often visit the place with higer posterior.**\n",
        "\n",
        "**How do you suggest the new direction to go once here in a certain position? **\n",
        "There are some algorithm such as Gibbs sampling (may not converge beacause of high dimenisonality because I can change I parameter at time, I cant move diagonally changing more parameter at every step), differential evolution, parallel tempering and simulated annealing.\n",
        "\n",
        "Our goal is to sample posterior distribution, and we use the emcee package. **Every algorithm maybe is better than other based on cavariance and dimensionality.**\n",
        "\n",
        "**And, once I've done, have I sample enough of posterior distribution?? (i.e. has mcmc converged?)** We have to check some things:\n",
        "\n",
        "- **the most important is to check autocorrelation of hte chain. The chain sould not be periodic.**\n",
        "- I have to check that all the chain converge in the same region, i.e. the chain runned enough time. \n",
        "- This means that the mean at beginning is equal to the mean at the end.\n",
        "- The entire chain has reached the stationary distribution, i.e. the entire chain reached the stationary distribution.\n",
        "\n",
        "Extracting the posterioir from the chain, means the assumption that the chain are independent.\n",
        "\n",
        "End on mcmc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HpCQlsOHNzM"
      },
      "source": [
        "#**MODEL SELECTION**\n",
        "\n",
        "High level discussion.\n",
        "\n",
        "The underline principle is the **principle of parsimony: we shouldn't postulate complexity if there is no need for it.**\n",
        "\n",
        "Between two theory and both explains the data  on just as well as the other, I will choose the one that is simpler. The most simple is the one with the fewer number of parameters. \n",
        "\n",
        "###**MODEL SELECTION METHODOLOGY**\n",
        "\n",
        "NHRT (he has all the porble of p-value statistic but it gives me an answer)\n",
        "\n",
        "AIC BIC MDL (bayesian statistic, more solid but it doesn't gives me an answer)\n",
        "\n",
        "A test to choose a model is the:\n",
        "\n",
        "**Likelihood-ratio test.** This test onyl works for nested model. For example, line fit with second degree term. One model is contained in the other. Then the likelihood ratio test says that: LR = -2log_e (L(complex model)/L(simple model)) is the pivotal quantity (i.e. assumption that under the null have a known distirbution), and it is chisq distributed under the null hypotesis. **The null is that the more complex model is overfitting the data.** In this case I will need to reject the null to say that the quadrati model is better than the line.\n",
        "\n",
        "This test applied to the mcmc with broken power law, gives me a very low value, so the broken power model is justified by hte data.\n",
        "\n",
        "Bayesyan method:\n",
        "\n",
        "The lower AIC is, the better the model is. \n",
        "\n",
        "The lower BIC is, the better the model is, but if I increase the number of parameters, BIC gets bigger.\n",
        "\n",
        "MDL test: harder to think the way before.\n",
        "\n",
        "Plotting the criterion and seeing the shape of the plot, I can see what is the best numebr of parameter in the point where the line goes down in the minimum.\n",
        "\n",
        "\n",
        "The data fot GRB are better fitted with a 4 degree polinomial model, rather than a line. But thhis model is not helpull in understand the way in which luminosity decreses. \n",
        "\n",
        "The AIC or BIC and MDL model are telling me that the gamma ray burst are contaminated by other thing that make the data deviate from a line fit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQY-cnLENHZQ"
      },
      "source": [
        "#CORE OF MACHINE LEARNING\n",
        "\n",
        "**We abandon things we need to talk about machine learning (such as general statistic, model selection, ..) for a while to study ONLY machine learning that is clustering, tree model, nearest neighbor, deep learning, ...**\n",
        "\n",
        "**Clustering** is the only unsupervised learning method we will explore, clustering and unsupervised learning are sinonymus but there are more method to do clustering. CLustering is a family of algorithm. \n",
        "\n",
        "Machine learning: adapting parameter of my model to the data. \n",
        "\n",
        "**Supervised learning:**\n",
        "- prediction\n",
        "- classification\n",
        "- feature selction\n",
        "\n",
        "**Unsueprvise learning**:\n",
        "- understaning the structue of the data in the feature space\n",
        "- organize the data\n",
        "- anomaly detection\n",
        "- dimenisonaltiy reduction\n",
        "\n",
        "\n",
        "Unsupervise learing is usefull to organize the data, compressing data, anomaly detection. \n",
        "Unsupervise learning: when i explore different model I wanna define if a model is interpretable or not. What are the consequence of my model even if my model was not created for other purpose. \"Interpretable\" means that I call tell you waht are the variable of the model are important to make a prediction (first) and maybe I can tell you which way this varaible can be used to make prediction (second). If there are some threshold those variable are important. \n",
        "\n",
        "They have to be interpretable because we ahve the responsability to create those model based on th eapplciation. AI is heavely used for complex social system and policy applcaitons. So they carry consequences on people. So an algorith need to be interpretable because one may want an explanation of the algorithm decision, if this decision are applied to him. \n",
        "\n",
        "We design an algorithm that makes the choice for me. And I want to see how algorithm makes the choice to make sure the model is unbaias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNuMuGic0HZ9"
      },
      "source": [
        "**As physicist: there is another issue that is much more overlooked about necessity for interpretable model. Which is: witouht the inrpretability of model I can't make causal connection, or necessarily clear correlation connection. And I can't say what I have predicted.**\n",
        "\n",
        "Another point is distinguish: paramter (thins that i learn from the data) and hypermparameters (things that I decide, and fix regardless of the data). \n",
        "Example: I choose to fit a line to my data (hyperparameter), more generally I choose the degree of the polinomial fit and the shape of the polinomial. \n",
        "\n",
        "In ML when we choose model we are free to look what are the parameter and hyperparameter that I choose. In python all is encoded, so if I don't choose parameter and hyperparameter, I choose to use the default settings. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1YoO03-3ZqM"
      },
      "source": [
        "**CLUSTERING**: I observe all the feature in my dataset, and I observe every variable for every object. I am not trying to predict a value that is harder to observe for future data, that is another problem. \n",
        "\n",
        "###**The goal is to split the feature space into a maximially homogeneous maximaly distinguscible subsets.**\n",
        "\n",
        "How should I group the observatoin in the feature space? How many clusters? That's big problem.\n",
        "\n",
        "\"Homogeneous\" and \"Distinguiscible\": I have internal criteiron that member of cluster should be similar to each other, and ecternal criterion that is outside the cluster should be dissimilar from the objects insde the cluster. (e.g. cow with cows, tiger with tiger,..)\n",
        "\n",
        "But I can also group things by another perspective: maybe I just care about the color of the picture, so I am interested in different group from the previuos groups.\n",
        "\n",
        "**Clustering is usefull also to compressing the data by replacing continuos variable with the cluster that is closer to the value of that variable. This is helpfull to understand edge of observation: usefull for image processing**\n",
        "\n",
        "Segmentation: understand the boundaries of object, that means understanding the structue of object. \n",
        "\n",
        "I treat images with np array since image are collection of number between 0 and 255, being one pixel a three color channel for R G and B. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGXhwOpw42ff"
      },
      "source": [
        "**For my cluster to be optimum I have to define a similarity or distance, and have a clear purpose for my clustering.**\n",
        "\n",
        "- My algorith should be scalable: they should work on large datasets with minimum loss of efficeincy in respect of small datasets.\n",
        "\n",
        "- My clustering will have the ability to deal with dirrent type of attributes: e.g. if I have numerical data it's not a problem, but what if I have categoriacl data? \n",
        "\n",
        "- Discover cluster of any shape\n",
        "\n",
        "- minimal requiring for domain knowledge\n",
        "\n",
        "- deals wiht noise and outliers, very important\n",
        "\n",
        "- insensitive to order in which I put things in group\n",
        "\n",
        "- inrpretable\n",
        "\n",
        "\n",
        "**So I have to define a distance in order to define similarity.** \n",
        "\n",
        "Numerical distances: one example is the Minkowski family of distances, which has some properties.\n",
        "If p=1 I get the manhattan city. If p=2 I get euclidian distance.\n",
        "\n",
        "There are also the distance on the sphere.\n",
        "\n",
        "###HOW WE DEFINE DISTANCE BETWEEN CATEGORICAL VARIABLE?\n",
        "\n",
        "If the varaible are binary (I have or don't have a feature) I will have a contingency table. This tells me all of the combination that I may have.\n",
        "\n",
        "So in this way I can use the \"sum\" shown in the table to build a distance function: **\"Simple MAtching Coefficient\" $\\rightarrow$ Simple MAtching DIstance**. \n",
        "\n",
        "I can also have **Jaccard similarity** (called IoU in depp learning). \n",
        "\n",
        "We have all implemented distances in scipy. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtuFMAwb-URT"
      },
      "source": [
        "If I have a categorical variable that is not binary, e.g. color: that's a probelm. In ML this is solved by a technique called One Hot Encoding: I create a single variable for each of the categories, so I force variable to become binary. \n",
        "\n",
        "The most common aproach in ML if I have 3 color: R G and B this is categorical variable with 3 outcome. the most commo aproach is to build 3 variable and allow each observation to have a binary value for each color. So my object that are red green or blue, will have only 1 in one the R G or B. That way I can define a distance. \n",
        "\n",
        "I created three variable and now they are binary: the problem is that my number of variable can blow up. Another problem is that I can only take one of the value between R G or B. that means that my varibale are not independent. This means that taking one of those value between R G or B I create correlation between other variable. (e.g. chosing only one color, I can create a coorelation between animal color and size, and that's not good because in ML most of the model require that variable to be not correlated).\n",
        "\n",
        "  I create a variable with my varibale and I have to comunicate this covariance to my model. "
      ]
    }
  ]
}