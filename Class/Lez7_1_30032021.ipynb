{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lez7.1 30032021.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWHeRE053mmFgbloHT4y3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedeleDelvecchio/MLPNS/blob/main/Class/Lez7_1_30032021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X27DtWdK1k7"
      },
      "source": [
        "**Dopo aver finito l'esercizio nella lezione 6, continuiamo studiando modelli, regressioni lineari e incertezze.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zILixlIHLA-B"
      },
      "source": [
        "When I choose a ML model, I have to do assumption. E.g. \"my data are gaussian\". How I can see if it is a good assumption? I go back NHRT and run KS test to look distibutoin of data, so this can tells me how solid my hypotesis is. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcTEaSul62Fr"
      },
      "source": [
        "##**TAXONOMY**\n",
        "\n",
        "Usually csv file are organized, (and packages are written) assuming that the onrder in which I organize the datasets of the file, is to have:\n",
        "\n",
        "- **observation on the x axis (each row is an observation)**\n",
        "- **feature (i.e. variable) on the y axis (i.e. each column is a feature)**\n",
        "\n",
        "**Features** : an individual measurable property or characteristic of a phenomenon being observed.\n",
        "\n",
        "**Observation** : A data point, row, or sample in a dataset. Another term for instance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y5clPqJLfJG"
      },
      "source": [
        "The first ML model we study is the linear regression.\n",
        "\n",
        "**What is ML?**\n",
        "\n",
        "**Field of study that gives computers the ability to learn without being explicitly programmed.** Actually we programm computer to make decision. \n",
        "\n",
        "ML is a subset of data science, a subset of techniques model and tool. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA3yey9EMMz_"
      },
      "source": [
        "We will se the supervised and unsupervised model. the majority of ML rely on a model that is a mathematical formula, but not all the model (e.g. tree mode doesnt rely on a mathematical formula). \n",
        "\n",
        "A mathematical model could be y=ax+b.\n",
        "Model varaible:x (time, location, ..).\n",
        "\n",
        "**MODEL**: mathematical formula with parameters\n",
        "\n",
        "**DATA** a set of observations\n",
        "\n",
        "The data comes in and exists indipendently on the model, and they also have uncertanties. It exist in the same feature space in which my model exists. So for example in model y=ax+b (1 dim) my measurement are for my x.\n",
        "\n",
        "For every et of parameters (for every a and b) there are an infinity value of solution. Some goes through my data very well, other so bad. \n",
        "\n",
        "**ML: How do I choose one of the right parameters? based on the data I have**\n",
        "\n",
        "This is extremely general: this includes NN, tree model.\n",
        "\n",
        "\n",
        "**The best way to think about is:**\n",
        "\n",
        "**A MODEL IS A LOW DIMENSIONAL REPRESNTATION OF A HIGHER DIMENSIONALITY DATASET**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4kakOt13wjb"
      },
      "source": [
        "We have 2 type of learning we can do:\n",
        "\n",
        "**Unsupervised learning - Supervised learning**\n",
        "\n",
        "Based on 4 question I will answer I choose my model:\n",
        "\n",
        "1. Understand the structure of feature space (i.e. structure of variable space) --> group variable in meaning ways.\n",
        "\n",
        "2. Classify observation, based on examples.\n",
        "Deep learning model that detects cars, object, particle, ...\n",
        "\n",
        "3. Regression. This means classification in a small range of value. Classification with infinitely small classes. So classification model can be adapted to do regression.\n",
        "\n",
        "4. \"Feature importance analysis\": understand which features are important in prediction. This is helpfull when I have a large feature space. (Used in rando forest, for example). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbYGAEhi9igO"
      },
      "source": [
        "UNSUPERVISED VS SUPERVISED MODEL:\n",
        "\n",
        "- **Unsupervised model:** means **CLUSTERING.**\n",
        "\n",
        "Partitioning the feature space so that the existing data is group according to some target function. In this case when I am clustering all of my feature are observed, for all my data-point. I am trying to organize in group **determined by the machine**. \n",
        "Example: if I have 2 feature x and y, (on a plane). Based on the definition of the distance of data, machine can do 2 grouop (see slide). \n",
        "\n",
        "**This is used in anomaly detection in physical probelms: find menaingfull groupingof observation distant from all of the existing group.** I may use it to:\n",
        "\n",
        "1.  understand the structure of the data. They can reveal other process from the studied one, for example, if I am studing supernovae. \n",
        "They reveal themselves through the spectra and we classify them into classes. This classes are human artifact, and maybe there are no classes or classes are wrong.\n",
        "\n",
        "2. anomaly detection If we really want to understand what is happening we have to make sure that those calsses exist. So we may want to group automatically the classes and maybe the outcome are different classes or no group at all. \n",
        "\n",
        "3. dimenisonality reduction: this means that if I have a coplex dataset, with thousand input variable it is a problem. If I can group object and collapse it in 1 variable (best case) or in a subset of this thousand variable, this can\n",
        "\n",
        "\n",
        "- **Supervised learning**: allows to do **classification and regression**: \n",
        "\n",
        "**finding  function of the variables that allow to predict the unobserved properties fo new observations. The datapoint for which the targer feature is oboserved are said to be \"labeled\"**\n",
        "\n",
        "I have a set of observations, for a subset of observarion I have all of the feature observed. While in another subset there are some feature not observed and I want to predit those missing features based on the observed one.\n",
        "\n",
        "Unilke unseupervises, here we have a large variety of aproach to do this. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2KGzbI8z9H8"
      },
      "source": [
        "##**TRAIN TEST AND VALIDATE**\n",
        "\n",
        "Linear regression is a ML method, but existed for a long time, but people were not doing ML.\n",
        "\n",
        "Whenever I run ML model I want to focus on validation of the model, based on some things like accuracy, precisoin, recall, ... \n",
        "\n",
        "When I run a linear regression for example, I will measure the **accuracy** calculating the chi^2 (quantity that we saw last week). \n",
        "\n",
        "But if I were in a ML framework I will do another step which is the **validation** of my model. Typically is done splittinn the label data (dat for which I have observed all the varibale) into **TRAINING** and **TEST** set.\n",
        "Typically training set contain 70% of the data, and the test set 30%.\n",
        "\n",
        "The parameter of the model are learned with the training set (majority of the data). But when I measure the accuracy or other quantity, I measure on data that the model has never seen before, i.e. the test set. \n",
        "\n",
        "The test is what I measure the accuracy on. I wanna demonstrate that the model works on data that he has never seen before. \n",
        "\n",
        "####**THIS IS CALLED CROSS VALIDATION**\n",
        "\n",
        "My model need to learn things in the data that are generalizable, and not specifics caratteristics of the sabsets of the data that I gave it. When my model learn things specifics of the data this is called an **overtraining**. We have overfitted the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U-7h5Vj3n8c"
      },
      "source": [
        "An upgraded version of this is the training, test and validation.\n",
        "\n",
        "After training, I run model on the test set to see if I overfitted the data, i.e. if I get a lower accuracy than I got on the training data. Then I modify the model, changing the hyperparameter, untill it performs as well as the test and traing test in the same way. \n",
        "\n",
        "**So I will have 3 splitting of the data:**\n",
        "- **training** to train the data\n",
        "- **test** to see if I have overfitted, and I modify the model until I no longer overfit\n",
        "- **validation** I measure the performance of the model on this set of data, and never modify the model again.\n",
        "\n",
        "**LINEAR REGRESSION**\n",
        "\n",
        "What if I have to fit a line to data? There are 2 reason: to predict of forecast. \n",
        "\n",
        "There are 2 type of varaible: \n",
        "\n",
        "###**INPUT VARIABLE, INDIEPNDENT VARIABLES OR EXOGENOUS**\n",
        "\n",
        "###**OUTPUT VARIABLE, DEPENDENT, PREDICTED OT ENDOGENOUS VARIABLE**\n",
        "\n",
        "1. **Predicting** the value of observation within the range of input varibale. \n",
        "**Forecasting**: I wanna extrapolate the model, i.e. predict things outside the exogenous variable. E.g. in time we want to forecast, used in global wariming, to see what will happen in the future.\n",
        "\n",
        "If i want to predict some value in between two value, this is predicting. \n",
        "\n",
        "If I want to predict the past is called hingcasting, to demonstrate that the model can predict the duture I predict the past.\n",
        "\n",
        "2. The other reason for linear regression is to explain the relationship between endogenous and exogenous variable, common in physics. Example this is used for the expansion of the universe, to validate a mathematical model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byd0LyRa6hTe"
      },
      "source": [
        "###ANALYTICAL SOLUTION OF LINEAR REGRESSION\n",
        "\n",
        "Can be demonstrated that I have a dataset of feature X, I can find what is the best line that fits the data entirely anlytically using linear algebra. We can do this using sklearn.linearRegression \n",
        "\n",
        "When I do linear regression I do other things, actually. I abandon the analytically solution because data have uncertanties, so I create an **objective function**.\n",
        "\n",
        "Objective function: this idea is the heart of ML. **Objective function is somethings that I minimize bases on the data, then I am left with what are the best parameters that fits my data**\n",
        "\n",
        "This is what I am doing when I minimize the chisquare, or the squared error, or the root mean square error. Those can be all objective function. In this way I can include uncertanties of the data. \n",
        "This is helpful to find the best sets of parameters. I use this when there is no analytical solutoin. \n",
        "\n",
        "##DISTANCE\n",
        "To find something that say what are the best parameters, I define typically the **distance** betweent the prediction and the data. Then I want to minimize this function to find the best parameters. The distance is a function and can be define in a huge number of ways. All distance are allowed, based on what I want to minimize. Typically we minimize the squared distance between observed data and prediction. \n",
        "\n",
        "The common distance are $L_1$ and $L_2$. \n",
        "Using $L_2$ I have more significance for the outliers!! Because I am squaring the distance of the outliers, which are by definition already distant from data.\n",
        "I can also use $\\chi^2$, that is $L_2/\\sigma^2$\n",
        "\n",
        "$\\rightarrow$ **$\\chi^2$ parameters is the exponent of a gaussian, so $\\chi^2$ is assuming gaussianity for my data.**\n",
        "\n",
        "Reading: https://arxiv.org/abs/1008.4686"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJyBPSw47j5h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}